{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6531bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:25:59.618292Z",
     "iopub.status.busy": "2022-01-23T02:25:59.616635Z",
     "iopub.status.idle": "2022-01-23T02:26:03.021510Z",
     "shell.execute_reply": "2022-01-23T02:26:03.020936Z",
     "shell.execute_reply.started": "2022-01-19T11:20:17.036084Z"
    },
    "papermill": {
     "duration": 3.439413,
     "end_time": "2022-01-23T02:26:03.021680",
     "exception": false,
     "start_time": "2022-01-23T02:25:59.582267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code copied from kaggle notebook, and made changes on top of it\n",
    "# https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/discussion/274970\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DATA_DIR = '../input'\n",
    "DATA_DIR = './datasets'\n",
    "\n",
    "# data configurations\n",
    "USE_PRECOMPUTE_FEATURES = True  # Load precomputed features for train.csv from private dataset (just for speed up)\n",
    "\n",
    "# model & ensemble configurations\n",
    "PREDICT_CNN = True\n",
    "PREDICT_MLP = True\n",
    "PREDICT_GBDT = True\n",
    "PREDICT_TABNET = False\n",
    "\n",
    "GBDT_NUM_MODELS = 3\n",
    "GBDT_LR = 0.02  # 0.1\n",
    "\n",
    "NN_VALID_TH = 0.185\n",
    "NN_MODEL_TOP_N = 3\n",
    "TAB_MODEL_TOP_N = 3\n",
    "ENSEMBLE_METHOD = 'mean'\n",
    "NN_NUM_MODELS = 10\n",
    "TABNET_NUM_MODELS = 5\n",
    "\n",
    "# for saving quota\n",
    "IS_1ST_STAGE = True\n",
    "SHORTCUT_NN_IN_1ST_STAGE = True  # early-stop training to save GPU quota\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = True\n",
    "MEMORY_TEST_MODE = False\n",
    "\n",
    "# for ablation studies\n",
    "CV_SPLIT = 'time'  # 'time': time-series KFold 'group': GroupKFold by stock-id\n",
    "USE_PRICE_NN_FEATURES = True  # Use nearest neighbor features that rely on tick size\n",
    "USE_VOL_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_SIZE_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_RANDOM_NN_FEATURES = False  # Use random index to aggregate neighbors\n",
    "\n",
    "USE_TIME_ID_NN = True  # Use time-id based neighbors\n",
    "USE_STOCK_ID_NN = True  # Use stock-id based neighbors\n",
    "\n",
    "ENABLE_RANK_NORMALIZATION = True  # Enable rank-normalization\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "    \n",
    "def print_trace(name: str = ''):\n",
    "    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "import pickle\n",
    "def pickle_dumps(file_name, data):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pickle_load(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def read_x_y():\n",
    "    X = pd.read_csv('./data-cache/X.csv')\n",
    "    y = pd.read_csv(\"./data-cache/y.csv\")\n",
    "\n",
    "    print('X.shape ', X.shape)\n",
    "    print('y.shape ', y.shape)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1540aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_df_into_train_test(df):\n",
    "    train_index = int(len(df) * 0.8)\n",
    "    train_data = df[:train_index]\n",
    "    test_data = df[train_index:]\n",
    "    print('Train data shape ', train_data.shape)\n",
    "    print('Test data shape ', test_data.shape)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def split_df_into_train_val_test(df):\n",
    "    # split 70, 15, 15\n",
    "    train_index = int(len(df) * 0.7)\n",
    "    train_data = df[:train_index]\n",
    "    val_test_data = df[train_index:]\n",
    "    val_index = int(len(val_test_data) * 0.5)\n",
    "    val_data = val_test_data[:val_index]\n",
    "    test_data = val_test_data[val_index:]\n",
    "    print('Total data shape ', df.shape)\n",
    "    print('train shape ', train_data.shape)\n",
    "    print('validation shape ', val_data.shape)\n",
    "    print('test shape ', test_data.shape)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "li = list(range(1,25))\n",
    "df_li = pd.DataFrame(li)\n",
    "t_df, v_df, te_df = split_df_into_train_val_test(df_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9283ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.105533Z",
     "iopub.status.busy": "2022-01-23T02:26:30.104748Z",
     "iopub.status.idle": "2022-01-23T02:26:30.415719Z",
     "shell.execute_reply": "2022-01-23T02:26:30.416330Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.625381Z"
    },
    "papermill": {
     "duration": 0.341799,
     "end_time": "2022-01-23T02:26:30.416539",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.074740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])\n",
    "print('Train.shape ', train.shape)\n",
    "print('stock_ids ', len(stock_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ids_to_include = [0]\n",
    "train = train[train['stock_id'].isin(stock_ids_to_include)]\n",
    "print('Train.shape ', train.shape)\n",
    "stock_ids = set(train['stock_id'])\n",
    "print('stock_ids ', stock_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85702d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e67a8c8",
   "metadata": {
    "papermill": {
     "duration": 0.030693,
     "end_time": "2022-01-23T02:26:30.479103",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.448410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bcf32",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.573125Z",
     "iopub.status.busy": "2022-01-23T02:26:30.553308Z",
     "iopub.status.idle": "2022-01-23T02:26:30.575466Z",
     "shell.execute_reply": "2022-01-23T02:26:30.575064Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.920189Z"
    },
    "papermill": {
     "duration": 0.067985,
     "end_time": "2022-01-23T02:26:30.575573",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.507588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataBlock(Enum):\n",
    "    TRAIN = 1\n",
    "    TEST = 2\n",
    "    BOTH = 3\n",
    "\n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', directory, f'stock_id={stock_id}'))\n",
    "\n",
    "def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "def load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'book', block)\n",
    "\n",
    "def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'trade', block)\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "def make_book_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'], group_keys=False)['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'], group_keys=False)['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'], group_keys=False)['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'], group_keys=False)['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'], group_keys=False)['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'], group_keys=False)['bid_price2'].apply(log_return)\n",
    "\n",
    "    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "    \n",
    "    features = {\n",
    "        'seconds_in_bucket': ['count'],\n",
    "        'wap1': [np.sum, np.mean, np.std],\n",
    "        'wap2': [np.sum, np.mean, np.std],\n",
    "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "        'wap_balance': [np.sum, np.mean, np.std],\n",
    "        'price_spread':[np.sum, np.mean, np.std],\n",
    "        'bid_spread':[np.sum, np.mean, np.std],\n",
    "        'ask_spread':[np.sum, np.mean, np.std],\n",
    "        'total_volume':[np.sum, np.mean, np.std],\n",
    "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "    }\n",
    "    \n",
    "    agg = book.groupby('time_id', group_keys=False).agg(features).reset_index(drop=False)\n",
    "    agg.columns = flatten_name('book', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "    \n",
    "    for time in [450, 300, 150]:\n",
    "        d = book[book['seconds_in_bucket'] >= time].groupby('time_id', group_keys=False).agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'book_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id', group_keys=False)['price'].apply(log_return)\n",
    "\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id', group_keys=False).agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    for time in [450, 300, 150]:\n",
    "        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "        d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "        agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        try:\n",
    "            price_list = prices.loc[tid].values.flatten()\n",
    "            price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "            ticks[tid] = price_diff[0]\n",
    "        except Exception:\n",
    "            print_trace(f'tid={tid}')\n",
    "            ticks[tid] = np.nan\n",
    "        \n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    return dst\n",
    "\n",
    "def make_features(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "        #df = make_extra_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6b213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.639029Z",
     "iopub.status.busy": "2022-01-23T02:26:30.638303Z",
     "iopub.status.idle": "2022-01-23T02:26:41.422225Z",
     "shell.execute_reply": "2022-01-23T02:26:41.421651Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.961136Z"
    },
    "papermill": {
     "duration": 10.818325,
     "end_time": "2022-01-23T02:26:41.422361",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.604036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "USE_PRECOMPUTE_FEATURES = False\n",
    "print('USE_PRECOMPUTE_FEATURES ', USE_PRECOMPUTE_FEATURES)\n",
    "if USE_PRECOMPUTE_FEATURES:\n",
    "    with timer('load feather'):\n",
    "        # df = pd.read_feather(os.path.join(DATA_DIR, 'optiver-df2', 'features_v2.f'))\n",
    "        df = pd.read_feather('./data-cache/features_v2.f')\n",
    "else:\n",
    "    print('making features ')\n",
    "    df = make_features(train, DataBlock.TRAIN)\n",
    "    # v2\n",
    "    df = make_features_v2(df, DataBlock.TRAIN)\n",
    "\n",
    "    df.to_feather('./data-cache/features_v2.f')  # save cache\n",
    "\n",
    "# test = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'test.csv'))\n",
    "# if len(test) == 3:\n",
    "    # print('is 1st stage')\n",
    "    # IS_1ST_STAGE = True\n",
    "\n",
    "# MEMORY_TEST_MODE = True\n",
    "# if IS_1ST_STAGE and MEMORY_TEST_MODE:\n",
    "#     print('use copy of training data as test data to immitate 2nd stage RAM usage.')\n",
    "#     test_df = df.iloc[:170000].copy()\n",
    "#     test_df['time_id'] += 32767\n",
    "#     test_df['row_id'] = ''\n",
    "# else:\n",
    "#     test_df = make_features(test, DataBlock.TEST)\n",
    "#     test_df = make_features_v2(test_df, DataBlock.TEST)\n",
    "\n",
    "print('data shape ', df.shape)\n",
    "# print(test_df.shape)\n",
    "# df = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e281cfe2",
   "metadata": {
    "papermill": {
     "duration": 0.028421,
     "end_time": "2022-01-23T02:26:41.480303",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.451882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca7436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.555136Z",
     "iopub.status.busy": "2022-01-23T02:26:41.550462Z",
     "iopub.status.idle": "2022-01-23T02:26:41.557458Z",
     "shell.execute_reply": "2022-01-23T02:26:41.557058Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.104849Z"
    },
    "papermill": {
     "duration": 0.048663,
     "end_time": "2022-01-23T02:26:41.557564",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.508901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 1 # 80\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        \n",
    "        if metric == 'random':\n",
    "            n_queries = len(pivot)\n",
    "            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n",
    "        else:\n",
    "            print('metric ', metric)\n",
    "            \n",
    "            nn = NearestNeighbors(\n",
    "                n_neighbors=N_NEIGHBORS_MAX, \n",
    "                p=p, \n",
    "                metric=metric, \n",
    "                metric_params=metric_params\n",
    "            )\n",
    "            # print('running NearestNeighbors ', nn)\n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,:], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.unstack().reset_index()\n",
    "        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n",
    "        return dst\n",
    "\n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "        feature_pivot.head()\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "\n",
    "\n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"stock-id based nearest neighbor features\"\"\"\n",
    "        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a076e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.619972Z",
     "iopub.status.busy": "2022-01-23T02:26:41.619012Z",
     "iopub.status.idle": "2022-01-23T02:26:41.982083Z",
     "shell.execute_reply": "2022-01-23T02:26:41.981564Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.127333Z"
    },
    "papermill": {
     "duration": 0.395558,
     "end_time": "2022-01-23T02:26:41.982243",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.586685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "df['trade.tau'] = np.sqrt(1 / df['trade.seconds_in_bucket.count'])\n",
    "df['trade_150.tau'] = np.sqrt(1 / df['trade_150.seconds_in_bucket.count'])\n",
    "df['book.tau'] = np.sqrt(1 / df['book.seconds_in_bucket.count'])\n",
    "df['real_price'] = 0.01 / df['tick_size']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be5a6240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T02:18:50.195022Z",
     "iopub.status.busy": "2022-01-16T02:18:50.1946Z",
     "iopub.status.idle": "2022-01-16T02:18:50.201136Z",
     "shell.execute_reply": "2022-01-16T02:18:50.199965Z",
     "shell.execute_reply.started": "2022-01-16T02:18:50.194964Z"
    },
    "papermill": {
     "duration": 0.030837,
     "end_time": "2022-01-23T02:26:42.050294",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.019457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Build Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aead97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:42.123778Z",
     "iopub.status.busy": "2022-01-23T02:26:42.122544Z",
     "iopub.status.idle": "2022-01-23T02:33:32.953387Z",
     "shell.execute_reply": "2022-01-23T02:33:32.953798Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.499414Z"
    },
    "papermill": {
     "duration": 410.874751,
     "end_time": "2022-01-23T02:33:32.953989",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.079238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_id_neighbors: List[Neighbors] = []\n",
    "stock_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "with timer('knn fit'):\n",
    "    df_pv = df[['stock_id', 'time_id']].copy()\n",
    "    df_pv['price'] = 0.01 / df['tick_size']\n",
    "    df_pv['vol'] = df['book.log_return1.realized_volatility']\n",
    "    df_pv['trade.tau'] = df['trade.tau']\n",
    "    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n",
    "\n",
    "    print('USE_PRICE_NN_FEATURES ', USE_PRICE_NN_FEATURES)\n",
    "    USE_PRICE_NN_FEATURES = False\n",
    "    if USE_PRICE_NN_FEATURES:\n",
    "        pivot = df_pv.pivot('time_id', 'stock_id', 'price')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_c', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='canberra', \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_price_m', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='mahalanobis',\n",
    "                metric_params={'VI':np.linalg.inv(np.cov(pivot.values.T))}\n",
    "            )\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_price_l1', \n",
    "                minmax_scale(pivot.transpose()), \n",
    "                p=1, \n",
    "                exclude_self=True)\n",
    "        )\n",
    "\n",
    "    print('USE_VOL_NN_FEATURES ', USE_VOL_NN_FEATURES)\n",
    "    USE_VOL_NN_FEATURES = False\n",
    "    if USE_VOL_NN_FEATURES:\n",
    "        pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors('time_vol_l1', pivot, p=1)\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_vol_l1', \n",
    "                minmax_scale(pivot.transpose()), \n",
    "                p=1, \n",
    "                exclude_self=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print('USE_SIZE_NN_FEATURES ', USE_SIZE_NN_FEATURES)\n",
    "    USE_SIZE_NN_FEATURES = False\n",
    "    if USE_SIZE_NN_FEATURES:\n",
    "        pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_m', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='mahalanobis', \n",
    "                # metric_params={'V':np.cov(pivot.values.T)}\n",
    "                metric_params={'VI':np.linalg.inv(np.cov(pivot.values.T))}\n",
    "            )\n",
    "        )\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_size_c', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='canberra'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    print('USE_RANDOM_NN_FEATURES ', USE_RANDOM_NN_FEATURES)\n",
    "    USE_RANDOM_NN_FEATURES = False\n",
    "    if USE_RANDOM_NN_FEATURES:\n",
    "        pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "        pivot = pivot.fillna(pivot.mean())\n",
    "        pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "        time_id_neighbors.append(\n",
    "            TimeIdNeighbors(\n",
    "                'time_random', \n",
    "                pivot, \n",
    "                p=2, \n",
    "                metric='random'\n",
    "            )\n",
    "        )\n",
    "        stock_id_neighbors.append(\n",
    "            StockIdNeighbors(\n",
    "                'stock_random', \n",
    "                pivot.transpose(), \n",
    "                p=2,\n",
    "                metric='random')\n",
    "        )\n",
    "\n",
    "\n",
    "print('USE_TIME_ID_NN ', USE_TIME_ID_NN)\n",
    "if not USE_TIME_ID_NN:\n",
    "    time_id_neighbors = []\n",
    "    \n",
    "print('USE_STOCK_ID_NN ', USE_STOCK_ID_NN)\n",
    "if not USE_STOCK_ID_NN:\n",
    "    stock_id_neighbors = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27029360",
   "metadata": {
    "papermill": {
     "duration": 0.028479,
     "end_time": "2022-01-23T02:33:33.011086",
     "exception": false,
     "start_time": "2022-01-23T02:33:32.982607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check Neighbor Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd5a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.073803Z",
     "iopub.status.busy": "2022-01-23T02:33:33.073080Z",
     "iopub.status.idle": "2022-01-23T02:33:33.075920Z",
     "shell.execute_reply": "2022-01-23T02:33:33.075471Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.548287Z"
    },
    "papermill": {
     "duration": 0.035942,
     "end_time": "2022-01-23T02:33:33.076032",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.040090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_rank_correraltion(neighbors, top_n=5):\n",
    "    if not neighbors:\n",
    "        return\n",
    "    neighbor_indices = pd.DataFrame()\n",
    "    for n in neighbors:\n",
    "        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n",
    "\n",
    "    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5123638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.138908Z",
     "iopub.status.busy": "2022-01-23T02:33:33.138122Z",
     "iopub.status.idle": "2022-01-23T02:33:33.192860Z",
     "shell.execute_reply": "2022-01-23T02:33:33.192422Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.555683Z"
    },
    "papermill": {
     "duration": 0.08837,
     "end_time": "2022-01-23T02:33:33.192975",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.104605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_ids = np.array(sorted(df['time_id'].unique()))\n",
    "for neighbor in time_id_neighbors:\n",
    "    print(neighbor)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            time_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(time_ids, name='time_id'), \n",
    "            # ALERT: NOTE value was 10 in range and was updated to 2\n",
    "            columns=[f'top_{i+1}' for i in range(2)] #10\n",
    "        ).iloc[1:6]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2589b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.263643Z",
     "iopub.status.busy": "2022-01-23T02:33:33.262935Z",
     "iopub.status.idle": "2022-01-23T02:33:33.276604Z",
     "shell.execute_reply": "2022-01-23T02:33:33.276173Z",
     "shell.execute_reply.started": "2022-01-19T11:39:40.610534Z"
    },
    "papermill": {
     "duration": 0.051151,
     "end_time": "2022-01-23T02:33:33.276704",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.225553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stock_ids = np.array(sorted(df['stock_id'].unique()))\n",
    "for neighbor in stock_id_neighbors:\n",
    "    print(neighbor)\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            stock_ids[neighbor.neighbors[:,:10]], \n",
    "            index=pd.Index(stock_ids, name='stock_id'), \n",
    "            # NOTE: range was 10,\n",
    "            columns=[f'top_{i+1}' for i in range(2)] #10\n",
    "        ).loc[0] #64\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b5471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.348457Z",
     "iopub.status.busy": "2022-01-23T02:33:33.347681Z",
     "iopub.status.idle": "2022-01-23T02:33:33.901994Z",
     "shell.execute_reply": "2022-01-23T02:33:33.902453Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.542166Z"
    },
    "papermill": {
     "duration": 0.591893,
     "end_time": "2022-01-23T02:33:33.902600",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.310707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate_rank_correraltion(time_id_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df3e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.038369Z",
     "iopub.status.busy": "2022-01-23T02:33:34.037754Z",
     "iopub.status.idle": "2022-01-23T02:33:34.275994Z",
     "shell.execute_reply": "2022-01-23T02:33:34.276373Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.949648Z"
    },
    "papermill": {
     "duration": 0.313195,
     "end_time": "2022-01-23T02:33:34.276516",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.963321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate_rank_correraltion(stock_id_neighbors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac11498c",
   "metadata": {
    "papermill": {
     "duration": 0.035477,
     "end_time": "2022-01-23T02:33:34.347529",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.312052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Aggregate Features With Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec957b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.427036Z",
     "iopub.status.busy": "2022-01-23T02:33:34.426197Z",
     "iopub.status.idle": "2022-01-23T02:33:36.057149Z",
     "shell.execute_reply": "2022-01-23T02:33:36.056625Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.189634Z"
    },
    "papermill": {
     "duration": 1.674163,
     "end_time": "2022-01-23T02:33:36.057283",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.383120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features with large changes over time are converted to relative ranks within time-id\n",
    "print('ENABLE_RANK_NORMALIZATION ', ENABLE_RANK_NORMALIZATION)\n",
    "if ENABLE_RANK_NORMALIZATION:\n",
    "    df['trade.order_count.mean'] = df.groupby('time_id', group_keys=False)['trade.order_count.mean'].rank()\n",
    "    df['book.total_volume.sum']  = df.groupby('time_id', group_keys=False)['book.total_volume.sum'].rank()\n",
    "    df['book.total_volume.mean'] = df.groupby('time_id', group_keys=False)['book.total_volume.mean'].rank()\n",
    "    df['book.total_volume.std']  = df.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "    df['trade.tau'] = df.groupby('time_id', group_keys=False)['trade.tau'].rank()\n",
    "\n",
    "    for dt in [150, 300, 450]:\n",
    "        df[f'book_{dt}.total_volume.sum']  = df.groupby('time_id', group_keys=False)[f'book_{dt}.total_volume.sum'].rank()\n",
    "        df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id', group_keys=False)[f'book_{dt}.total_volume.mean'].rank()\n",
    "        df[f'book_{dt}.total_volume.std']  = df.groupby('time_id', group_keys=False)[f'book_{dt}.total_volume.std'].rank()\n",
    "        df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id', group_keys=False)[f'trade_{dt}.order_count.mean'].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc7d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.151666Z",
     "iopub.status.busy": "2022-01-23T02:33:36.150004Z",
     "iopub.status.idle": "2022-01-23T02:33:36.152354Z",
     "shell.execute_reply": "2022-01-23T02:33:36.152748Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.199422Z"
    },
    "papermill": {
     "duration": 0.059468,
     "end_time": "2022-01-23T02:33:36.152899",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.093431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    print('df2.shape', df2.shape)\n",
    "\n",
    "    feature_cols_stock = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_150.tau': [np.mean],\n",
    "        'book.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "    }\n",
    "    \n",
    "    feature_cols = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'real_price': [np.max, np.mean, np.min],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n",
    "        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "    stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "\n",
    "    # neighbor stock_id\n",
    "    for feature_col in feature_cols_stock.keys():\n",
    "        try:\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            if not stock_id_neighbors:\n",
    "                continue\n",
    "\n",
    "            for nn in stock_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "            for agg in feature_cols_stock[feature_col]:\n",
    "                for n in stock_id_neighbor_sizes:\n",
    "                    try:\n",
    "                        for nn in stock_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('stock-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('stock-id nn')\n",
    "            pass\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    ndf = None\n",
    "\n",
    "    print(df2.shape)\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try:\n",
    "            if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n",
    "                continue\n",
    "            if feature_col not in df2.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df2, feature_col)\n",
    "\n",
    "            if 'volatility' in feature_col:\n",
    "                time_id_ns = time_id_neigbor_sizes_vol\n",
    "            else:\n",
    "                time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "    # features further derived from nearest neighbor features\n",
    "    try:\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes:\n",
    "                denominator = f\"real_price_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'real_price_rankmin_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'real_price_rankmax_{sz}']  = df2['real_price'] / df2[f\"{denominator}_amax\"]\n",
    "                df2[f'real_price_rankmean_{sz}'] = df2['real_price'] / df2[f\"{denominator}_mean\"]\n",
    "\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n",
    "\n",
    "                df2[f'vol_rankmin_{sz}'] = \\\n",
    "                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amin\"]\n",
    "                df2[f'vol_rankmax_{sz}'] = \\\n",
    "                    df2['book.log_return1.realized_volatility'] / df2[f\"{denominator}_amax\"]\n",
    "\n",
    "        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "        for c in price_cols:\n",
    "            del df2[c]\n",
    "\n",
    "        if USE_PRICE_NN_FEATURES:\n",
    "            for sz in time_id_neigbor_sizes_vol:\n",
    "                tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n",
    "                df2[f'{tgt}_rank'] = df2.groupby('time_id', group_keys=False)[tgt].rank()\n",
    "    except Exception:\n",
    "        print_trace('nn features')\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a6c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.370271Z",
     "iopub.status.busy": "2022-01-23T02:33:36.368938Z",
     "iopub.status.idle": "2022-01-23T02:34:51.959576Z",
     "shell.execute_reply": "2022-01-23T02:34:51.959095Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.224929Z"
    },
    "papermill": {
     "duration": 75.77123,
     "end_time": "2022-01-23T02:34:51.959705",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.188475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "with timer('make nearest neighbor feature'):\n",
    "    df2 = make_nearest_neighbor_feature(df)\n",
    "\n",
    "print(df2.shape)\n",
    "df2.reset_index(drop=True).to_feather('./data-cache/optiver_df2.f')\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "049e3ff0",
   "metadata": {
    "papermill": {
     "duration": 0.037563,
     "end_time": "2022-01-23T02:34:52.038355",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.000792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Misc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672b203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.076612Z",
     "iopub.status.busy": "2022-01-23T02:34:53.075859Z",
     "iopub.status.idle": "2022-01-23T02:34:53.335135Z",
     "shell.execute_reply": "2022-01-23T02:34:53.334610Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.290787Z"
    },
    "papermill": {
     "duration": 1.258742,
     "end_time": "2022-01-23T02:34:53.335258",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.076516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# skew correction for NN\n",
    "cols_to_log = [\n",
    "    'trade.size.sum',\n",
    "    'trade_150.size.sum',\n",
    "    'trade_300.size.sum',\n",
    "    'trade_450.size.sum',\n",
    "    'volume_imbalance'\n",
    "]\n",
    "for c in df2.columns:\n",
    "    for check in cols_to_log:\n",
    "        try:\n",
    "            if check in c:\n",
    "                df2[c] = np.log(df2[c]+1)\n",
    "                break\n",
    "        except Exception:\n",
    "            print_trace('log1p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b80e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.415634Z",
     "iopub.status.busy": "2022-01-23T02:34:53.414883Z",
     "iopub.status.idle": "2022-01-23T02:34:54.757020Z",
     "shell.execute_reply": "2022-01-23T02:34:54.756480Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.724354Z"
    },
    "papermill": {
     "duration": 1.384579,
     "end_time": "2022-01-23T02:34:54.757155",
     "exception": false,
     "start_time": "2022-01-23T02:34:53.372576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rolling average of RV for similar trading volume\n",
    "try:\n",
    "    df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    roll_target = 'book.log_return1.realized_volatility'\n",
    "\n",
    "    for window_size in [3, 10]:\n",
    "        df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n",
    "            df2.groupby('stock_id', group_keys=False)[roll_target].rolling(window_size, center=True, min_periods=1) \\\n",
    "                                                .mean() \\\n",
    "                                                .reset_index() \\\n",
    "                                                .sort_values(by=['level_1'])[roll_target].values\n",
    "except Exception:\n",
    "    print_trace('mean RV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16725fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv.pivot('time_id', 'stock_id', 'vol').columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d35d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790434b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pv.pivot('time_id', 'stock_id', 'vol').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac17ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:54.865738Z",
     "iopub.status.busy": "2022-01-23T02:34:54.859008Z",
     "iopub.status.idle": "2022-01-23T02:34:57.630440Z",
     "shell.execute_reply": "2022-01-23T02:34:57.631709Z",
     "shell.execute_reply.started": "2022-01-15T04:54:08.318718Z"
    },
    "papermill": {
     "duration": 2.836215,
     "end_time": "2022-01-23T02:34:57.631962",
     "exception": false,
     "start_time": "2022-01-23T02:34:54.795747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # stock-id embedding (helps little)\n",
    "# try:\n",
    "#     lda_n = 3\n",
    "#     lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "\n",
    "#     stock_id_emb = pd.DataFrame(\n",
    "#         lda.fit_transform(pivot.transpose()), \n",
    "#         index=df_pv.pivot('time_id', 'stock_id', 'vol').columns\n",
    "#     )\n",
    "\n",
    "#     for i in range(lda_n):\n",
    "#         df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
    "# except Exception:\n",
    "#     print_trace('LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.target.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df2 shape ', df2.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f898c5d1",
   "metadata": {
    "papermill": {
     "duration": 0.037142,
     "end_time": "2022-01-23T02:34:59.516263",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.479121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reverse Engineering time-id Order & Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306bfda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.616772Z",
     "iopub.status.busy": "2022-01-23T02:34:59.615063Z",
     "iopub.status.idle": "2022-01-23T02:34:59.617414Z",
     "shell.execute_reply": "2022-01-23T02:34:59.617868Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.7715Z"
    },
    "papermill": {
     "duration": 0.063706,
     "end_time": "2022-01-23T02:34:59.618003",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.554297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "    \n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id', group_keys=False).apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        # book_path = '/kaggle/input/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet'\n",
    "        book_path = './datasets/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet'\n",
    "        df_files = pd.DataFrame(\n",
    "            {'book_path': glob.glob(book_path)}) \\\n",
    "            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ad6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.705424Z",
     "iopub.status.busy": "2022-01-23T02:34:59.704651Z",
     "iopub.status.idle": "2022-01-23T02:35:01.635920Z",
     "shell.execute_reply": "2022-01-23T02:35:01.636972Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.801275Z"
    },
    "papermill": {
     "duration": 1.981013,
     "end_time": "2022-01-23T02:35:01.637181",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.656168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('CV_SPLIT ', CV_SPLIT)\n",
    "USE_PRECOMPUTE_FEATURES = False\n",
    "print('USE_PRECOMPUTE_FEATURES ', USE_PRECOMPUTE_FEATURES)\n",
    "if CV_SPLIT == 'time':\n",
    "    with timer('calculate order of time-id'):\n",
    "        if USE_PRECOMPUTE_FEATURES:\n",
    "            # timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\n",
    "            timeid_order = pd.read_csv(\"./data-cache/time_id_order.csv\")\n",
    "        else:\n",
    "            timeid_order = reconstruct_time_id_order()\n",
    "            \n",
    "    with timer('make folds'):\n",
    "        timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "        df2['time_id_order'] = df2['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "        df2 = df2.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "        # folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n",
    "        # time_id_orders = df2['time_id_order']\n",
    "\n",
    "        # folds = []\n",
    "        # for i, border in enumerate(folds_border):\n",
    "        #     idx_train = np.where(time_id_orders < border)[0]\n",
    "        #     idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n",
    "        #     folds.append((idx_train, idx_valid))\n",
    "        #     print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n",
    "\n",
    "    # del df2['time_id_order']\n",
    "elif CV_SPLIT == 'group':\n",
    "    gkf = GroupKFold(n_splits=4)\n",
    "    # folds = []\n",
    "\n",
    "    # for i, (idx_train, idx_valid) in enumerate(gkf.split(df2, None, groups=df2['time_id'])):\n",
    "    #     folds.append((idx_train, idx_valid))\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee818419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validation, df_test = split_df_into_train_val_test(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df2, df_pv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3269d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287161cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee88bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_id_ordered_plot(stock_id, df, first_n_records = None):\n",
    "    df_train_per_stock = df[df['stock_id'] == stock_id]\n",
    "    if first_n_records:\n",
    "        df_train_per_stock = df_train_per_stock[0: first_n_records]\n",
    "    print('df_train_per_stock.shape',df_train_per_stock.shape)\n",
    "    plt.plot(range(len(df_train_per_stock)), df_train_per_stock['target'])\n",
    "    plt.title('Time Id ordered plot of target')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Realized volatility')\n",
    "    plt.title('Reealized volatility for stock ' + str(stock_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68994df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_id_ordered_plot(0, df_train, 36*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_id_ordered_plot(0, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds\n",
    "# print('folds len ', len(folds))\n",
    "# len(folds[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "\n",
    "def add_model_result(model_name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    model_result_light_gbm = [m for m in model_results if m['model_name'] == model_name]\n",
    "    if model_result_light_gbm:\n",
    "        print('value already exists in model results. So updating it')\n",
    "        for model in model_results:\n",
    "            if model['model_name'] == model_name:\n",
    "                model['mse'] = mse\n",
    "                model['mae'] = mae\n",
    "                model['rmse'] = rmse\n",
    "                model['added_date'] = datetime.now()\n",
    "    else:\n",
    "        print('adding new model results in')\n",
    "        model_results.append({'model_name': model_name, 'mse': mse, 'rmse': rmse, 'mae': mae, 'added_date': datetime.now()})\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "020eecf0",
   "metadata": {
    "papermill": {
     "duration": 0.067715,
     "end_time": "2022-01-23T02:35:01.777174",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.709459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6f703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:01.921148Z",
     "iopub.status.busy": "2022-01-23T02:35:01.920367Z",
     "iopub.status.idle": "2022-01-23T02:35:01.933964Z",
     "shell.execute_reply": "2022-01-23T02:35:01.934928Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.902446Z"
    },
    "papermill": {
     "duration": 0.091675,
     "end_time": "2022-01-23T02:35:01.935102",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.843427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # order by average importance across folds\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # plot top-n\n",
    "    PLOT_TOP_N = 50\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c35af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = GBDT_LR\n",
    "print('SHORTCUT_GBDT_IN_1ST_STAGE ', SHORTCUT_GBDT_IN_1ST_STAGE)\n",
    "print('IS_1ST_STAGE ', IS_1ST_STAGE)\n",
    "if SHORTCUT_GBDT_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "    # to save GPU quota\n",
    "    lr = 0.3\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0,\n",
    "    'metric': '',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 5,\n",
    "    'min_data_in_leaf': 1000,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'learning_rate': lr\n",
    "}\n",
    "\n",
    "X_train = get_X(df_train)\n",
    "X_val = get_X(df_validation)\n",
    "X_test = get_X(df_test)\n",
    "\n",
    "y_train = df_train['target']\n",
    "y_val = df_validation['target']\n",
    "y_test = df_test['target']\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb222a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:02.048565Z",
     "iopub.status.busy": "2022-01-23T02:35:02.044618Z",
     "iopub.status.idle": "2022-01-23T03:35:55.320275Z",
     "shell.execute_reply": "2022-01-23T03:35:55.319782Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.922483Z"
    },
    "papermill": {
     "duration": 3653.32245,
     "end_time": "2022-01-23T03:35:55.320410",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.997960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PREDICT_GBDT = True\n",
    "print('PREDICT_GBDT ', PREDICT_GBDT)\n",
    "if PREDICT_GBDT:\n",
    "    ds = lgb.Dataset(X_train, y_train, weight=1/np.power(y_train, 2))\n",
    "\n",
    "    with timer('lgb.cv'):\n",
    "        ret = lgb.cv(params, ds, num_boost_round=8000, \n",
    "                     #folds=folds, #cv,\n",
    "                     feval=feval_RMSPE, \n",
    "                     stratified=False, \n",
    "                     return_cvbooster=True, \n",
    "                     verbose_eval=20,\n",
    "                     early_stopping_rounds=int(40*0.1/lr))\n",
    "\n",
    "        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "    best_iteration = len(ret['RMSPE-mean'])\n",
    "\n",
    "    print('boosters length ', len(ret['cvbooster'].boosters))\n",
    "    y_pred = ret['cvbooster'].boosters[0].predict(X_val, num_iteration=best_iteration)\n",
    "    print(f\"RMSPE: {rmspe(y_val, y_pred)}\")\n",
    "\n",
    "    print(add_model_result('LightGBM', y_val, y_pred))\n",
    "\n",
    "    # for i in range(len(folds)):\n",
    "    #     y_pred = ret['cvbooster'].boosters[i].predict(X_train.iloc[folds[i][1]], num_iteration=best_iteration)\n",
    "    #     y_true = y_train.iloc[folds[i][1]]\n",
    "    #     print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n",
    "        \n",
    "    #     if i == len(folds) - 1:\n",
    "    #         np.save('./data-cache/pred_gbdt.npy', y_pred)\n",
    "\n",
    "    plot_importance(ret['cvbooster'], figsize=(10, 20))\n",
    "\n",
    "    boosters = []\n",
    "    with timer('retraining'):\n",
    "        for i in range(GBDT_NUM_MODELS):\n",
    "            params['seed'] = i\n",
    "            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n",
    "\n",
    "    booster = EnsembleModel(boosters)\n",
    "    del ret\n",
    "    del ds\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSPE: 0.32889500000000005\n",
    "model_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb015e20",
   "metadata": {
    "papermill": {
     "duration": 0.057064,
     "end_time": "2022-01-23T03:35:55.434906",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.377842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce13e5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:55.569557Z",
     "iopub.status.busy": "2022-01-23T03:35:55.558558Z",
     "iopub.status.idle": "2022-01-23T03:35:56.706846Z",
     "shell.execute_reply": "2022-01-23T03:35:56.705908Z",
     "shell.execute_reply.started": "2022-01-15T04:57:16.2193Z"
    },
    "papermill": {
     "duration": 1.211778,
     "end_time": "2022-01-23T03:35:56.706992",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.495214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "NUM_WORKERS = 0 #4\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        # self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print('TabularDataset __getitem__ idx ', idx)\n",
    "        # print('x_num length ', len(self.x_num))\n",
    "        # print('y length ', len(self.y))\n",
    "        if self.y is None:\n",
    "            # print('returning only x_num')\n",
    "            return self.x_num[idx]\n",
    "            # return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            # print('returning xnum and y')\n",
    "            return self.x_num[idx], self.y[idx]\n",
    "            # return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                #  n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                #  emb_dim: int = 10,\n",
    "                #  dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.embs = nn.ModuleList([\n",
    "        #     nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        # self.cat_dim = emb_dim * len(n_categories)\n",
    "        # self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                # nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Linear(src_num_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                # nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Linear(src_num_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    # def forward(self, x_num, x_cat):\n",
    "    def forward(self, x_num):\n",
    "        # embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        # x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        # x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_num)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                #  n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        # self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            # nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Dropout(dropout_top),\n",
    "            # nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        # self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        # self.cat_dim = emb_dim * len(n_categories)\n",
    "        # self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    # def forward(self, x_num, x_cat):\n",
    "    def forward(self, x_num):\n",
    "        # embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        # x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        # x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x_num)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "# def preprocess_nn(\n",
    "#         X: pd.DataFrame,\n",
    "#         scaler: Optional[StandardScaler] = None,\n",
    "#         scaler_type: str = 'standard',\n",
    "#         n_pca: int = -1,\n",
    "#         na_cols: bool = True):\n",
    "#     if na_cols:\n",
    "#         #for c in X.columns:\n",
    "#         for c in null_check_cols:\n",
    "#             if c in X.columns:\n",
    "#                 X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "#     cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "#     num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "#     X_num = X[num_cols].values.astype(np.float32)\n",
    "#     X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "#     def _pca(X_num_):\n",
    "#         if n_pca > 0:\n",
    "#             pca = PCA(n_components=n_pca, random_state=0)\n",
    "#             return pca.fit_transform(X_num)\n",
    "#         return X_num\n",
    "\n",
    "#     if scaler is None:\n",
    "#         scaler = StandardScaler()\n",
    "#         X_num = scaler.fit_transform(X_num)\n",
    "#         X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "#         return _pca(X_num), X_cat, cat_cols, scaler\n",
    "#     else:\n",
    "#         X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "#         X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "#         return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    # print('calling model train')\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    # print('looping through data loader ')\n",
    "    # print('Not using tqdm')\n",
    "    for x_num, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "    # for x_num, x_cat, y in data_loader: #, position=0, leave=True, desc='Training'):\n",
    "        # print('x_num ', len(x_num))\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        # x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        # print('calculating rmspe loss')\n",
    "        loss = rmspe_loss(y, model(x_num))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # print('not using tqdm ')\n",
    "        for x_num, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "        # for x_num, x_cat, y in data_loader: #, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            # x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # output = model(x_num, x_cat)\n",
    "                output = model(x_num)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               scaler: StandardScaler,\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    # X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "    # valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_dataset = TabularDataset(X_val.values, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=NUM_WORKERS)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "        for x_num in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            # x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    # output = m(x_num, x_cat)\n",
    "                    output = m(x_num)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "# def predict_tabnet(X: pd.DataFrame,\n",
    "#                    model: Union[List[TabNetRegressor], TabNetRegressor],\n",
    "#                    scaler: StandardScaler,\n",
    "#                    ensemble_method='mean'):\n",
    "#     if not isinstance(model, list):\n",
    "#         model = [model]\n",
    "\n",
    "#     X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n",
    "#     X_processed = np.concatenate([X_cat, X_num], axis=1)\n",
    "\n",
    "#     predicted = []\n",
    "#     for m in model:\n",
    "#         predicted.append(m.predict(X_processed))\n",
    "\n",
    "#     if ensemble_method == 'median':\n",
    "#         pred = np.nanmedian(np.array(predicted), axis=0)\n",
    "#     else:\n",
    "#         pred = np.array(predicted).mean(axis=0)\n",
    "\n",
    "#     return pred\n",
    "\n",
    "\n",
    "# def train_tabnet(X: pd.DataFrame,\n",
    "                #  y: pd.DataFrame,\n",
    "                # #  folds: List[Tuple],\n",
    "                #  batch_size: int = 1024,\n",
    "                #  lr: float = 1e-3,\n",
    "                #  model_path: str = 'fold_{}.pth',\n",
    "                #  scaler_type: str = 'standard',\n",
    "                #  output_dir: str = 'artifacts',\n",
    "                #  epochs: int = 250,\n",
    "                #  seed: int = 42,\n",
    "                #  n_pca: int = -1,\n",
    "                #  na_cols: bool = True,\n",
    "                #  patience: int = 10,\n",
    "                #  factor: float = 0.5,\n",
    "                #  gamma: float = 2.0,\n",
    "                #  lambda_sparse: float = 8.0,\n",
    "                #  n_steps: int = 2,\n",
    "                #  scheduler_type: str = 'cosine',\n",
    "                #  n_a: int = 16):\n",
    "    # seed_everything(seed)\n",
    "\n",
    "    # os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # y = y.values.astype(np.float32)\n",
    "    # X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    # best_losses = []\n",
    "    # best_predictions = []\n",
    "\n",
    "    # for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "    # X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "    # X_tr = X_train.copy()\n",
    "    # X_va = X_val.copy()\n",
    "    # X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "    # y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "    # y_tr = y_train.copy()\n",
    "    # y_va = y_val.copy()\n",
    "    # y_tr = y_tr.reshape(-1,1)\n",
    "    # y_va = y_va.reshape(-1,1)\n",
    "    # X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n",
    "    # X_va = np.concatenate([X_va_cat, X_va], axis=1)\n",
    "\n",
    "    # cat_idxs = [0]\n",
    "    # cat_dims = [128]\n",
    "\n",
    "    # if scheduler_type == 'cosine':\n",
    "    #     scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n",
    "    #     scheduler_fn = CosineAnnealingWarmRestarts\n",
    "    # else:\n",
    "    #     scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n",
    "    #     scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "\n",
    "    # model = TabNetRegressor(\n",
    "    #     cat_idxs=cat_idxs,\n",
    "    #     cat_dims=cat_dims,\n",
    "    #     cat_emb_dim=1,\n",
    "    #     n_d=n_a,\n",
    "    #     n_a=n_a,\n",
    "    #     n_steps=n_steps,\n",
    "    #     gamma=gamma,\n",
    "    #     n_independent=2,\n",
    "    #     n_shared=2,\n",
    "    #     lambda_sparse=lambda_sparse,\n",
    "    #     optimizer_fn=torch.optim.Adam,\n",
    "    #     optimizer_params={'lr': lr},\n",
    "    #     mask_type=\"entmax\",\n",
    "    #     scheduler_fn=scheduler_fn,\n",
    "    #     scheduler_params=scheduler_params,\n",
    "    #     seed=seed,\n",
    "    #     verbose=10\n",
    "    #     #device_name=device,\n",
    "    #     #clip_value=1.5\n",
    "    # )\n",
    "\n",
    "    # print('NUM_WORKERS before model fit ', NUM_WORKERS)\n",
    "    # model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n",
    "    #             virtual_batch_size=batch_size, num_workers=NUM_WORKERS, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n",
    "\n",
    "    # path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "    # model.save_model(path)\n",
    "\n",
    "    # predicted = model.predict(X_va)\n",
    "\n",
    "    # rmspe = rmspe_metric(y_va, predicted)\n",
    "    # best_losses.append(rmspe)\n",
    "    # best_predictions.append(predicted)\n",
    "\n",
    "    # return best_losses, best_predictions, scaler, model\n",
    "\n",
    "\n",
    "def train_nn(\n",
    "        # X: pd.DataFrame,\n",
    "            #  y: pd.DataFrame,\n",
    "            #  folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # y = y.values.astype(np.float32)\n",
    "    # X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    # for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "    # X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "    # X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "    # y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "    cur_batch = batch_size\n",
    "    best_loss = 1e10\n",
    "    best_prediction = None\n",
    "\n",
    "    # print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "    train_dataset = TabularDataset(X_train.values, y_train.values)\n",
    "    valid_dataset = TabularDataset(X_val.values, y_val.values)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                num_workers=NUM_WORKERS)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                num_workers=NUM_WORKERS)\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        # print('creating MLP object')\n",
    "        model = MLP(X_train.shape[1],\n",
    "                    # n_categories=[128],\n",
    "                    dropout=mlp_dropout, \n",
    "                    hidden=mlp_hidden, \n",
    "                    # emb_dim=emb_dim,\n",
    "                    # dropout_cat=dropout_emb, \n",
    "                    bn=mlp_bn)\n",
    "    elif model_type == 'cnn':\n",
    "        model = CNN(X_train.shape[1],\n",
    "                    hidden_size=cnn_hidden,\n",
    "                    # n_categories=[128],\n",
    "                    emb_dim=emb_dim,\n",
    "                    dropout_cat=dropout_emb,\n",
    "                    channel_1=cnn_channel1,\n",
    "                    channel_2=cnn_channel2,\n",
    "                    channel_3=cnn_channel3,\n",
    "                    two_stage=False,\n",
    "                    kernel1=cnn_kernel1,\n",
    "                    celu=cnn_celu,\n",
    "                    dropout_top=cnn_dropout,\n",
    "                    dropout_mid=cnn_dropout,\n",
    "                    dropout_bottom=cnn_dropout,\n",
    "                    weight_norm=cnn_weight_norm,\n",
    "                    leaky_relu=cnn_leaky_relu)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    model = model.to(device)\n",
    "\n",
    "    if optimizer_type == 'adamw':\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'adam':\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    scheduler = epoch_scheduler = None\n",
    "    if scheduler_type == 'onecycle':\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                        max_lr=max_lr, epochs=epochs,\n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "    elif scheduler_type == 'reduce':\n",
    "        epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                        mode='min',\n",
    "                                                                        min_lr=1e-7,\n",
    "                                                                        patience=patience,\n",
    "                                                                        verbose=True,\n",
    "                                                                        factor=factor)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "            cur_batch = cur_batch * 2\n",
    "            print(f'batch: {cur_batch}')\n",
    "            # print('NUM_WORKERS here ', NUM_WORKERS)\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                        batch_size=cur_batch,\n",
    "                                                        shuffle=False,\n",
    "                                                        num_workers=NUM_WORKERS)\n",
    "        # print('Training for epoch ', epoch)\n",
    "        train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "        predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "        print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "        if epoch_scheduler is not None:\n",
    "            epoch_scheduler.step(rmspe)\n",
    "\n",
    "        if rmspe < best_loss:\n",
    "            print(f'new best:{rmspe}')\n",
    "            best_loss = rmspe\n",
    "            best_prediction = predictions\n",
    "            # torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n",
    "\n",
    "    best_predictions.append(best_prediction)\n",
    "    best_losses.append(best_loss)\n",
    "    # del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "    del model, train_dataset, valid_dataset, train_loader, valid_loader, opt\n",
    "    if scheduler is not None:\n",
    "        del scheduler\n",
    "    gc.collect()\n",
    "    # , scaler\n",
    "    return best_losses, best_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_name():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    \n",
    "    return \"cpu\"\n",
    "device = torch.device(get_device_name())\n",
    "print('device', device)\n",
    "\n",
    "# del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('number of models are less than top_n. all models will be used')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SHORTCUT_NN_IN_1ST_STAGE ', SHORTCUT_NN_IN_1ST_STAGE)\n",
    "print('IS_1ST_STAGE ', IS_1ST_STAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12245cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:57.029334Z",
     "iopub.status.busy": "2022-01-23T03:35:56.898214Z",
     "iopub.status.idle": "2022-01-23T06:56:27.693883Z",
     "shell.execute_reply": "2022-01-23T06:56:27.695021Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.584692Z"
    },
    "papermill": {
     "duration": 12030.930352,
     "end_time": "2022-01-23T06:56:27.695345",
     "exception": false,
     "start_time": "2022-01-23T03:35:56.764993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "PREDICT_MLP = True\n",
    "print('PREDICT_MLP ', PREDICT_MLP)\n",
    "if PREDICT_MLP:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "    \n",
    "    print('SHORTCUT_NN_IN_1ST_STAGE ', SHORTCUT_NN_IN_1ST_STAGE)\n",
    "    print('IS_1ST_STAGE ', IS_1ST_STAGE)\n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 30\n",
    "        valid_th = NN_VALID_TH\n",
    "        \n",
    "    NN_NUM_MODELS = 1\n",
    "    print('NN_NUM_MODELS ', NN_NUM_MODELS)\n",
    "    \n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        # MLP\n",
    "        nn_losses, nn_preds = train_nn(\n",
    "            # X_train, y_train, \n",
    "                                            #    [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               batch_size=512,\n",
    "                                               mlp_bn=True,\n",
    "                                               mlp_hidden=256,\n",
    "                                               mlp_dropout=0.0,\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs,\n",
    "                                               lr=0.002,\n",
    "                                               max_lr=0.0055,\n",
    "                                               weight_decay=1e-7,\n",
    "                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i)\n",
    "        print('NN_VALID_TH ', valid_th)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            print(f'model of seed {i} added.')\n",
    "            scores.append(nn_losses[0])\n",
    "            model_paths.append(f'artifacts/mlp_fold_0_seed{i}.pth')\n",
    "            np.save(f'./data-cache/pred_mlp_seed{i}.npy', nn_preds[0])\n",
    "\n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    mlp_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(mlp_model)} models will be used.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4acd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PREDICT_CNN = True\n",
    "print('PREDICT_CNN ', PREDICT_CNN)\n",
    "\n",
    "print('NN_NUM_MODELS ', NN_NUM_MODELS)\n",
    "\n",
    "if PREDICT_CNN:\n",
    "    model_paths = []\n",
    "    scores = []\n",
    "        \n",
    "    print('SHORTCUT_NN_IN_1ST_STAGE ', SHORTCUT_NN_IN_1ST_STAGE)\n",
    "    print('IS_1ST_STAGE ', IS_1ST_STAGE)\n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 100\n",
    "    else:\n",
    "        epochs = 50\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(NN_NUM_MODELS):\n",
    "        nn_losses, nn_preds = train_nn(\n",
    "            # X, y, \n",
    "                                            #    [folds[-1]], \n",
    "                                               device=device, \n",
    "                                               cnn_hidden=8*128,\n",
    "                                               batch_size=1280,\n",
    "                                               model_type='cnn',\n",
    "                                               emb_dim=30,\n",
    "                                               epochs=epochs, #epochs,\n",
    "                                               cnn_channel1=128,\n",
    "                                               cnn_channel2=3*128,\n",
    "                                               cnn_channel3=3*128,\n",
    "                                               lr=0.00038, #0.0011,\n",
    "                                               max_lr=0.0013,\n",
    "                                               weight_decay=6.5e-6,\n",
    "                                               optimizer_type='adam',\n",
    "                                               scheduler_type='reduce',\n",
    "                                               model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n",
    "                                               seed=i,\n",
    "                                               cnn_dropout=0.0,\n",
    "                                               cnn_weight_norm=False, # Note: True\n",
    "                                               cnn_leaky_relu=False,\n",
    "                                               patience=8,\n",
    "                                               factor=0.3)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            model_paths.append(f'artifacts/cnn_fold_0_seed{i}.pth')\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'./data-cache/pred_cnn_seed{i}.npy', nn_preds[0])\n",
    "            \n",
    "    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n",
    "    cnn_model = [torch.load(path, device) for path in model_paths]\n",
    "    print(f'total {len(cnn_model)} models will be used.')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f60943",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SHORTCUT_NN_IN_1ST_STAGE ', SHORTCUT_NN_IN_1ST_STAGE)\n",
    "print('IS_1ST_STAGE ', IS_1ST_STAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_TABNET = True\n",
    "print('PREDICT_TABNET ', PREDICT_TABNET)\n",
    "SHORTCUT_NN_IN_1ST_STAGE = True\n",
    "print('SHORTCUT_NN_IN_1ST_STAGE ', SHORTCUT_NN_IN_1ST_STAGE)\n",
    "print('IS_1ST_STAGE ', IS_1ST_STAGE)\n",
    "if PREDICT_TABNET:\n",
    "    tab_model = []\n",
    "    scores = []\n",
    "        \n",
    "    print('SHORTCUT_NN_IN_1ST_STAGE ', SHORTCUT_NN_IN_1ST_STAGE)\n",
    "    print('IS_1ST_STAGE ', IS_1ST_STAGE)\n",
    "    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n",
    "        print('shortcut to save quota...')\n",
    "        epochs = 3\n",
    "        valid_th = 1000\n",
    "    else:\n",
    "        print('train full')\n",
    "        epochs = 250\n",
    "        valid_th = NN_VALID_TH\n",
    "\n",
    "    for i in range(TABNET_NUM_MODELS):\n",
    "        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n",
    "                                                          [folds[-1]], \n",
    "                                                          batch_size=1280,\n",
    "                                                          epochs=epochs, #epochs,\n",
    "                                                          lr=0.04,\n",
    "                                                          patience=50,\n",
    "                                                          factor=0.5,\n",
    "                                                          gamma=1.6,\n",
    "                                                          lambda_sparse=3.55e-6,\n",
    "                                                          seed=i,\n",
    "                                                          n_a=36)\n",
    "        if nn_losses[0] < valid_th:\n",
    "            tab_model.append(model)\n",
    "            scores.append(nn_losses[0])\n",
    "            np.save(f'./data-cache/pred_tab_seed{i}.npy', nn_preds[0])\n",
    "            model.save_model(f'artifacts/tabnet_fold_0_seed{i}')\n",
    "            \n",
    "    tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n",
    "    print(f'total {len(tab_model)} models will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d484ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_TEMPORAL_CONVOLUTION_NETWORK = True\n",
    "print('PREDICT_TEMPORAL_CONVOLUTION_NETWORK ', PREDICT_TEMPORAL_CONVOLUTION_NETWORK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60247b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = read_x_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TCNModel, RNNModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.metrics import mape, r2_score\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset, EnergyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89fd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = X.isna().any()\n",
    "xp_null = xp.loc[lambda x : x == True]\n",
    "nan_columns = list(xp_null.index)\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=nan_columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614f6343",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = X.isna().any()\n",
    "xp_null = xp.loc[lambda x : x == True]\n",
    "nan_columns = list(xp_null.index)\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91913bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts_2 = TimeSeries.from_dataframe(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16113b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ts_2.columns)\n",
    "print(len(X_ts_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts = TimeSeries.from_dataframe(X)\n",
    "scaler_x_ts = Scaler()\n",
    "X_ts = scaler_x_ts.fit_transform(\n",
    "    X_ts\n",
    ").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c57f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts.to_pickle('./data-cache/X_ts.pickle', pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e843083",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e61c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts = TimeSeries.from_dataframe(y)\n",
    "scaler_ts_y = Scaler()\n",
    "y_ts = scaler_ts_y.fit_transform(\n",
    "    y_ts\n",
    ").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68822d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts.to_pickle('./data-cache/y_ts.pickle', pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ts = TimeSeries.from_pickle('./data-cache/X_ts.pickle')\n",
    "y_ts = TimeSeries.from_pickle('./data-cache/y_ts.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets:\n",
    "train_x_ts, val_x_ts = X_ts.split_after(0.8)\n",
    "train_y_ts, val_y_ts = y_ts.split_after(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length train_x_ts', len(train_x_ts))\n",
    "print('length val_x_ts', len(val_x_ts))\n",
    "print('length train_y_ts', len(train_y_ts))\n",
    "print('length val_y_ts', len(val_y_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c02cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef939fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tcn = TCNModel(\n",
    "    input_chunk_length=72,\n",
    "    output_chunk_length=36,\n",
    "    n_epochs=2, #500\n",
    "    dropout=0.1,\n",
    "    dilation_base=2,\n",
    "    weight_norm=True,\n",
    "    kernel_size=3,\n",
    "    num_filters=3,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "model_tcn.fit(\n",
    "    series=train_y_ts,\n",
    "    past_covariates=train_x_ts,\n",
    "    val_series=val_y_ts,\n",
    "    val_past_covariates=val_x_ts,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "backtest_tcn = model_tcn.historical_forecasts(\n",
    "    series=y_ts,\n",
    "    past_covariates=X_ts,\n",
    "    start=0.8,\n",
    "    forecast_horizon=6,\n",
    "    retrain=False,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tcn = model_tcn.predict(2)\n",
    "pred_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c163772",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "backtest_tcn = model_tcn.historical_forecasts(\n",
    "    series=y_ts,\n",
    "    past_covariates=X_ts,\n",
    "    start=0.8,\n",
    "    forecast_horizon=6,\n",
    "    retrain=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(backtest_tcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e3fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mae\n",
    "\n",
    "p = mae(backtest_tcn, y_ts)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.append({'model': 'TCN Multi', 'mae': 0.060581528})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcd975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.append({'model': 'TCN', 'mae': 0.056109045})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts.plot(label=\"actual\")\n",
    "backtest_tcn.plot(label=\"backtest (H=6)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score\n",
    "from darts.datasets import EnergyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6adc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_forecast(pred_series, ts_transformed, forecast_type, start_date=None):\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     if start_date:\n",
    "#         ts_transformed = ts_transformed.drop_before(start_date)\n",
    "#     ts_transformed.univariate_component(0).plot(label=\"actual\")\n",
    "#     pred_series.plot(label=(\"historic \" + forecast_type + \" forecasts\"))\n",
    "#     plt.title(\n",
    "#         \"R2: {}\".format(r2_score(ts_transformed.univariate_component(0), pred_series))\n",
    "#     )\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8863f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nbeats = NBEATSModel(\n",
    "#     input_chunk_length=30,\n",
    "#     output_chunk_length=7,\n",
    "#     generic_architecture=True,\n",
    "#     num_stacks=10,\n",
    "#     num_blocks=1,\n",
    "#     num_layers=4,\n",
    "#     layer_widths=512,\n",
    "#     n_epochs=1, # 100\n",
    "#     nr_epochs_val_period=1,\n",
    "#     batch_size=800,\n",
    "#     model_name=\"nbeats_run\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_nbeats.fit(train, val_series=val, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc61d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel, ExponentialSmoothing, BlockRNNModel\n",
    "from darts.metrics import mape\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "import darts.utils.timeseries_generation as tg\n",
    "from darts.datasets import AirPassengersDataset, EnergyDataset\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = RNNModel(\n",
    "    model=\"LSTM\",\n",
    "    hidden_dim=20,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    n_epochs=2,\n",
    "    optimizer_kwargs={\"lr\": 1e-3},\n",
    "    random_state=0,\n",
    "    training_length=300,\n",
    "    input_chunk_length=300,\n",
    "    likelihood=GaussianLikelihood(),\n",
    ")\n",
    "\n",
    "# model_en.fit(series=train_en_transformed, future_covariates=train_day, verbose=True)\n",
    "model_lstm.fit(\n",
    "    series=train_y_ts,\n",
    "    future_covariates=train_x_ts,\n",
    "    val_series=val_y_ts,\n",
    "    val_future_covariates=val_x_ts,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm = model_lstm.historical_forecasts(\n",
    "    series=y_ts,\n",
    "    future_covariates=X_ts,\n",
    "    num_samples=50,\n",
    "    start=0.8,\n",
    "    forecast_horizon=36,\n",
    "    stride=5,\n",
    "    retrain=False,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mae\n",
    "\n",
    "p = mae(backtest_lstm, y_ts)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.append({'model': 'LSTM Multi', 'mae': 0.03790627705864608})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b054246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.append({'model': 'LSTM', 'mae': 0.07019737421534955})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b007a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts.plot(label=\"actual\")\n",
    "backtest_lstm.plot(label=\"backtest (H=6)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = [{'model': 'Light GBM Multivariate', 'rmspe': 0.32889500000000005},\n",
    " {'model': 'TCN Multivariate', 'mae': 0.060581528},\n",
    " {'model': 'LSTM Multivariate', 'mae': 0.03790627705864608},\n",
    " {'model': 'TCN Univariate', 'mae': 0.056109045},\n",
    " {'model': 'LSTM Univariate', 'mae': 0.07019737421534955}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838bff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9363c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb97e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TransformerModel\n",
    "model_transformer = TransformerModel(\n",
    "    input_chunk_length=12,\n",
    "    output_chunk_length=1,\n",
    "    batch_size=32,\n",
    "    n_epochs=2,\n",
    "    model_name=\"air_transformer\",\n",
    "    nr_epochs_val_period=10,\n",
    "    d_model=16,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1,\n",
    "    activation=\"relu\",\n",
    "    random_state=42,\n",
    "    save_checkpoints=True,\n",
    "    force_reset=True,\n",
    ")\n",
    "\n",
    "model_transformer.fit(\n",
    "    series=train_y_ts,\n",
    "    past_covariates=train_x_ts,\n",
    "    val_series=val_y_ts,\n",
    "    val_past_covariates=val_x_ts,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# my_model.fit(series=train_scaled, val_series=val_scaled, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d669634",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_transformer = model_transformer.historical_forecasts(\n",
    "    series=y_ts,\n",
    "    past_covariates=X_ts,\n",
    "    start=0.8,\n",
    "    forecast_horizon=6,\n",
    "    retrain=False,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# # backtest_lstm = model_lstm.historical_forecasts(\n",
    "# #     series=y_ts,\n",
    "# #     future_covariates=X_ts,\n",
    "# #     num_samples=50,\n",
    "# #     start=0.8,\n",
    "# #     forecast_horizon=36,\n",
    "# #     stride=5,\n",
    "# #     retrain=False,\n",
    "# #     verbose=False,\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.metrics import mae\n",
    "\n",
    "mae_transformer = mae(backtest_transformer, y_ts)\n",
    "print(mae_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.append({'model': 'Transformer', 'mae': 0.05275301})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read data:\n",
    "# ts = AirPassengersDataset().load()\n",
    "# print('type of ts ')\n",
    "# scaler = Scaler()\n",
    "# ts = scaler.fit_transform(\n",
    "#     ts\n",
    "# )  # scale the whole time series not caring about train/val split...\n",
    "\n",
    "\n",
    "# # We'll use the month as a covariate\n",
    "# month_series = datetime_attribute_timeseries(ts, attribute=\"month\", one_hot=True)\n",
    "# scaler_month = Scaler()\n",
    "# month_series = scaler_month.fit_transform(month_series)\n",
    "\n",
    "# # Create training and validation sets:\n",
    "# train, val = ts.split_after(pd.Timestamp(\"19580801\"))\n",
    "# train_month, val_month = month_series.split_after(pd.Timestamp(\"19580801\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3a1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310b139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:57:18.576429Z",
     "iopub.status.busy": "2022-01-23T06:57:18.575179Z",
     "iopub.status.idle": "2022-01-23T06:57:18.578624Z",
     "shell.execute_reply": "2022-01-23T06:57:18.579071Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.81586Z"
    },
    "papermill": {
     "duration": 25.719783,
     "end_time": "2022-01-23T06:57:18.579245",
     "exception": false,
     "start_time": "2022-01-23T06:56:52.859462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del X, y\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ecb26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eab5e90",
   "metadata": {
    "papermill": {
     "duration": 25.580305,
     "end_time": "2022-01-23T06:58:09.549193",
     "exception": false,
     "start_time": "2022-01-23T06:57:43.968888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9627f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:58:59.994590Z",
     "iopub.status.busy": "2022-01-23T06:58:59.994023Z",
     "iopub.status.idle": "2022-01-23T06:58:59.998606Z",
     "shell.execute_reply": "2022-01-23T06:58:59.998183Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.009945Z"
    },
    "papermill": {
     "duration": 24.922124,
     "end_time": "2022-01-23T06:58:59.998728",
     "exception": false,
     "start_time": "2022-01-23T06:58:35.076604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = get_X(df_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b678419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T06:59:51.678805Z",
     "iopub.status.busy": "2022-01-23T06:59:51.677926Z",
     "iopub.status.idle": "2022-01-23T06:59:52.468053Z",
     "shell.execute_reply": "2022-01-23T06:59:52.468570Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.025123Z"
    },
    "papermill": {
     "duration": 26.521254,
     "end_time": "2022-01-23T06:59:52.468717",
     "exception": false,
     "start_time": "2022-01-23T06:59:25.947463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame()\n",
    "df_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "prediction_weights = {}\n",
    "\n",
    "print('PREDICT_GBDT ', PREDICT_GBDT)\n",
    "if PREDICT_GBDT:\n",
    "    gbdt_preds = booster.predict(X_test)\n",
    "    predictions['gbdt'] = gbdt_preds\n",
    "    prediction_weights['gbdt'] = 4\n",
    "\n",
    "print('PREDICT_MLP ', PREDICT_MLP)\n",
    "if PREDICT_MLP and mlp_model:\n",
    "    try:\n",
    "        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'mlp: {mlp_preds.shape}')\n",
    "        predictions['mlp'] = mlp_preds\n",
    "        prediction_weights['mlp'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict mlp: {traceback.format_exc()}')\n",
    "\n",
    "print('PREDICT_CNN ', PREDICT_CNN)\n",
    "if PREDICT_CNN and cnn_model:\n",
    "    try:\n",
    "        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "        print(f'cnn: {cnn_preds.shape}')\n",
    "        predictions['cnn'] = cnn_preds\n",
    "        prediction_weights['cnn'] = 4\n",
    "    except:\n",
    "        print(f'failed to predict cnn: {traceback.format_exc()}')\n",
    "\n",
    "print('PREDICT_TABNET ', PREDICT_TABNET)\n",
    "if PREDICT_TABNET and tab_model:\n",
    "    try:\n",
    "        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten()\n",
    "        print(f'tab: {tab_preds.shape}')\n",
    "        predictions['tab'] = tab_preds\n",
    "        prediction_weights['tab'] = 1\n",
    "    except:\n",
    "        print(f'failed to predict tab: {traceback.format_exc()}')\n",
    "\n",
    "        \n",
    "overall_preds = None\n",
    "overall_weight = np.sum(list(prediction_weights.values()))\n",
    "\n",
    "print(f'prediction will be made by: {list(prediction_weights.keys())}')\n",
    "\n",
    "for name, preds in predictions.items():\n",
    "    w = prediction_weights[name] / overall_weight\n",
    "    if overall_preds is None:\n",
    "        overall_preds = preds * w\n",
    "    else:\n",
    "        overall_preds += preds * w\n",
    "        \n",
    "df_pred['target'] = np.clip(overall_preds, 0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a77be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T07:00:43.579503Z",
     "iopub.status.busy": "2022-01-23T07:00:43.578812Z",
     "iopub.status.idle": "2022-01-23T07:00:43.602164Z",
     "shell.execute_reply": "2022-01-23T07:00:43.602580Z",
     "shell.execute_reply.started": "2022-01-15T04:57:18.056985Z"
    },
    "papermill": {
     "duration": 25.584209,
     "end_time": "2022-01-23T07:00:43.602727",
     "exception": false,
     "start_time": "2022-01-23T07:00:18.018518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\n",
    "submission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\n",
    "submission['target'] = submission['target'].fillna(0)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16517.385856,
   "end_time": "2022-01-23T07:01:10.301218",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-23T02:25:52.915362",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
