{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9510c8cf",
   "metadata": {},
   "source": [
    "\n",
    "code copied from kaggle notebook, and made changes on top of it  \n",
    "https://www.kaggle.com/competitions/optiver-realized-volatility-prediction/discussion/274970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e45c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "from joblib import delayed, Parallel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from datetime import datetime\n",
    "from darts import TimeSeries\n",
    "\n",
    "# from darts.models import RNNModel\n",
    "# from darts.models import  TransformerModel\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.metrics import mape, r2_score\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.datasets import AirPassengersDataset, SunspotsDataset, EnergyDataset\n",
    "from darts.metrics import mae, rmse, mse, mape\n",
    "import random\n",
    "from typing import List, Tuple, Optional, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.optim as optim\n",
    "import shutil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from darts.utils.statistics import check_seasonality, plot_acf\n",
    "import darts.utils.timeseries_generation as tg\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.missing_values import fill_missing_values\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DATA_DIR = '../input'\n",
    "# DATA_DIR = './datasets'\n",
    "DATA_DIR = '/Users/pujanmaharjan/uni adelaide/uofa_research_project/datasets'\n",
    "\n",
    "# data configurations\n",
    "USE_PRECOMPUTE_FEATURES = True  # Load precomputed features for train.csv from private dataset (just for speed up)\n",
    "\n",
    "# model & ensemble configurations\n",
    "PREDICT_CNN = True\n",
    "PREDICT_MLP = True\n",
    "PREDICT_GBDT = True\n",
    "PREDICT_TABNET = False\n",
    "\n",
    "GBDT_NUM_MODELS = 3\n",
    "GBDT_LR = 0.02  # 0.1\n",
    "\n",
    "NN_VALID_TH = 0.185\n",
    "NN_MODEL_TOP_N = 3\n",
    "TAB_MODEL_TOP_N = 3\n",
    "ENSEMBLE_METHOD = 'mean'\n",
    "NN_NUM_MODELS = 10\n",
    "TABNET_NUM_MODELS = 5\n",
    "\n",
    "# for saving quota\n",
    "IS_1ST_STAGE = True\n",
    "SHORTCUT_NN_IN_1ST_STAGE = True  # early-stop training to save GPU quota\n",
    "SHORTCUT_GBDT_IN_1ST_STAGE = True\n",
    "MEMORY_TEST_MODE = False\n",
    "\n",
    "# for ablation studies\n",
    "CV_SPLIT = 'time'  # 'time': time-series KFold 'group': GroupKFold by stock-id\n",
    "USE_PRICE_NN_FEATURES = True  # Use nearest neighbor features that rely on tick size\n",
    "USE_VOL_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_SIZE_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\n",
    "USE_RANDOM_NN_FEATURES = False  # Use random index to aggregate neighbors\n",
    "\n",
    "USE_TIME_ID_NN = True  # Use time-id based neighbors\n",
    "USE_STOCK_ID_NN = True  # Use stock-id based neighbors\n",
    "\n",
    "ENABLE_RANK_NORMALIZATION = True  # Enable rank-normalization\n",
    "\n",
    "EPOCHS = 2\n",
    "SEED = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55fc3539",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/pujanmaharjan/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN2at8internal15invoke_parallelExxxRKNSt3__18functionIFvxxEEE\n  Referenced from: <2E2DC0F1-5772-35B0-9D52-A746E12B51DE> /Users/pujanmaharjan/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <9DDA9B08-547F-32F9-87F1-796977059897> /Users/pujanmaharjan/anaconda/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/darts/models/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregression_ensemble_model\u001b[39;00m \u001b[39mimport\u001b[39;00m RegressionEnsembleModel\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregression_model\u001b[39;00m \u001b[39mimport\u001b[39;00m RegressionModel\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtbats_model\u001b[39;00m \u001b[39mimport\u001b[39;00m BATS, TBATS\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtheta\u001b[39;00m \u001b[39mimport\u001b[39;00m FourTheta, Theta\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvarima\u001b[39;00m \u001b[39mimport\u001b[39;00m VARIMA\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/darts/models/forecasting/block_rnn_model.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m get_logger, raise_if_not\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpl_forecasting_module\u001b[39;00m \u001b[39mimport\u001b[39;00m PLPastCovariatesModule\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdarts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforecasting\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch_forecasting_model\u001b[39;00m \u001b[39mimport\u001b[39;00m PastCovariatesTorchModel\n\u001b[1;32m     15\u001b[0m logger \u001b[39m=\u001b[39m get_logger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/darts/models/forecasting/pl_forecasting_module.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m ABC, abstractmethod\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Dict, Optional, Sequence, Tuple, Union\n\u001b[0;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/pytorch_lightning/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m     _logger\u001b[39m.\u001b[39mpropagate \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseed\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m LightningDataModule, LightningModule  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/pytorch_lightning/callbacks/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_size_finder\u001b[39;00m \u001b[39mimport\u001b[39;00m BatchSizeFinder\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcheckpoint\u001b[39;00m \u001b[39mimport\u001b[39;00m Checkpoint\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/pytorch_lightning/callbacks/batch_size_finder.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtuner\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_size_scaling\u001b[39;00m \u001b[39mimport\u001b[39;00m _scale_batch_size\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m _TunerExitException, MisconfigurationException\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/pytorch_lightning/callbacks/callback.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m Optimizer\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m STEP_OUTPUT\n\u001b[1;32m     25\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCallback\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Abstract base class used to build new callbacks.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[39m    Subclass this class and override any of the relevant hooks\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/pytorch_lightning/utilities/types.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m Metric\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightning_fabric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m _TORCH_LRSCHEDULER, LRScheduler, ProcessGroup, ReduceLROnPlateau\n\u001b[1;32m     29\u001b[0m _NUMBER \u001b[39m=\u001b[39m Union[\u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/__init__.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m _PACKAGE_ROOT \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(\u001b[39m__file__\u001b[39m)\n\u001b[1;32m     12\u001b[0m _PROJECT_ROOT \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(_PACKAGE_ROOT)\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m functional  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maggregation\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     CatMetric,\n\u001b[1;32m     17\u001b[0m     MaxMetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     SumMetric,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_deprecated\u001b[39;00m \u001b[39mimport\u001b[39;00m _PermutationInvariantTraining \u001b[39mas\u001b[39;00m PermutationInvariantTraining  \u001b[39m# noqa: E402\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/functional/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_deprecated\u001b[39;00m \u001b[39mimport\u001b[39;00m _permutation_invariant_training \u001b[39mas\u001b[39;00m permutation_invariant_training\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_deprecated\u001b[39;00m \u001b[39mimport\u001b[39;00m _pit_permutate \u001b[39mas\u001b[39;00m pit_permutate\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_deprecated\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     _scale_invariant_signal_distortion_ratio \u001b[39mas\u001b[39;00m scale_invariant_signal_distortion_ratio,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/functional/audio/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpit\u001b[39;00m \u001b[39mimport\u001b[39;00m permutation_invariant_training, pit_permutate\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msdr\u001b[39;00m \u001b[39mimport\u001b[39;00m scale_invariant_signal_distortion_ratio, signal_distortion_ratio\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msnr\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     complex_scale_invariant_signal_noise_ratio,\n\u001b[1;32m     18\u001b[0m     scale_invariant_signal_noise_ratio,\n\u001b[1;32m     19\u001b[0m     signal_noise_ratio,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/functional/audio/pit.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Literal\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimports\u001b[39;00m \u001b[39mimport\u001b[39;00m _SCIPY_AVAILABLE\n\u001b[1;32m     26\u001b[0m \u001b[39m# _ps_dict: cache of permutations\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# it's necessary to cache it, otherwise it will consume a large amount of time\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/utilities/__init__.py:14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright The Lightning team.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchecks\u001b[39;00m \u001b[39mimport\u001b[39;00m check_forward_full_state_property\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_to_collection\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m class_reduce, reduce\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/utilities/checks.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetric\u001b[39;00m \u001b[39mimport\u001b[39;00m Metric\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m select_topk, to_onehot\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menums\u001b[39;00m \u001b[39mimport\u001b[39;00m DataType\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/metric.py:30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[0;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     _flatten,\n\u001b[1;32m     32\u001b[0m     _squeeze_if_scalar,\n\u001b[1;32m     33\u001b[0m     apply_to_collection,\n\u001b[1;32m     34\u001b[0m     dim_zero_cat,\n\u001b[1;32m     35\u001b[0m     dim_zero_max,\n\u001b[1;32m     36\u001b[0m     dim_zero_mean,\n\u001b[1;32m     37\u001b[0m     dim_zero_min,\n\u001b[1;32m     38\u001b[0m     dim_zero_sum,\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributed\u001b[39;00m \u001b[39mimport\u001b[39;00m gather_all_tensors\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchMetricsUserError\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/utilities/data.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchMetricsUserWarning\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimports\u001b[39;00m \u001b[39mimport\u001b[39;00m _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprints\u001b[39;00m \u001b[39mimport\u001b[39;00m rank_zero_warn\n\u001b[1;32m     25\u001b[0m METRIC_EPS \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchmetrics/utilities/imports.py:48\u001b[0m\n\u001b[1;32m     46\u001b[0m _GAMMATONE_AVAILABEL: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m package_available(\u001b[39m\"\u001b[39m\u001b[39mgammatone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m _TORCHAUDIO_AVAILABEL: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m package_available(\u001b[39m\"\u001b[39m\u001b[39mtorchaudio\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m _TORCHAUDIO_GREATER_EQUAL_0_10: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m compare_version(\u001b[39m\"\u001b[39;49m\u001b[39mtorchaudio\u001b[39;49m\u001b[39m\"\u001b[39;49m, operator\u001b[39m.\u001b[39;49mge, \u001b[39m\"\u001b[39;49m\u001b[39m0.10.0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     49\u001b[0m _SACREBLEU_AVAILABLE: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m package_available(\u001b[39m\"\u001b[39m\u001b[39msacrebleu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m _REGEX_AVAILABLE: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m package_available(\u001b[39m\"\u001b[39m\u001b[39mregex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/lightning_utilities/core/imports.py:73\u001b[0m, in \u001b[0;36mcompare_version\u001b[0;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compare package version with some requirements.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m>>> compare_version(\"torch\", operator.ge, \"0.1\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     pkg \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(package)\n\u001b[1;32m     74\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mImportError\u001b[39;00m, pkg_resources\u001b[39m.\u001b[39mDistributionNotFound):\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     _extension,\n\u001b[1;32m      3\u001b[0m     compliance,\n\u001b[1;32m      4\u001b[0m     datasets,\n\u001b[1;32m      5\u001b[0m     functional,\n\u001b[1;32m      6\u001b[0m     io,\n\u001b[1;32m      7\u001b[0m     kaldi_io,\n\u001b[1;32m      8\u001b[0m     models,\n\u001b[1;32m      9\u001b[0m     pipelines,\n\u001b[1;32m     10\u001b[0m     sox_effects,\n\u001b[1;32m     11\u001b[0m     transforms,\n\u001b[1;32m     12\u001b[0m     utils,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m get_audio_backend, list_audio_backends, set_audio_backend\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/_extension/__init__.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m _IS_KALDI_AVAILABLE \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m _IS_TORCHAUDIO_EXT_AVAILABLE:\n\u001b[0;32m---> 43\u001b[0m     _load_lib(\u001b[39m\"\u001b[39;49m\u001b[39mlibtorchaudio\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_torchaudio\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     _check_cuda_version()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/_extension/utils.py:61\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[1;32m     62\u001b[0m torch\u001b[39m.\u001b[39mclasses\u001b[39m.\u001b[39mload_library(path)\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/site-packages/torch/_ops.py:643\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    638\u001b[0m path \u001b[39m=\u001b[39m _utils_internal\u001b[39m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m    639\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    640\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[1;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[1;32m    373\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/pujanmaharjan/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so, 0x0006): Symbol not found: __ZN2at8internal15invoke_parallelExxxRKNSt3__18functionIFvxxEEE\n  Referenced from: <2E2DC0F1-5772-35B0-9D52-A746E12B51DE> /Users/pujanmaharjan/anaconda/anaconda3/lib/python3.10/site-packages/torchaudio/lib/libtorchaudio.so\n  Expected in:     <9DDA9B08-547F-32F9-87F1-796977059897> /Users/pujanmaharjan/anaconda/anaconda3/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib"
     ]
    }
   ],
   "source": [
    "import darts.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c04bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in import\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from darts.models import TCNModel, RNNModel, TransformerModel\n",
    "except:\n",
    "    print('Error in import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1540aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_into_train_test(df):\n",
    "    train_index = int(len(df) * 0.8)\n",
    "    train_data = df[:train_index]\n",
    "    test_data = df[train_index:]\n",
    "    print('Train data shape ', train_data.shape)\n",
    "    print('Test data shape ', test_data.shape)\n",
    "    return train_data, test_data\n",
    "\n",
    "def split_df_into_train_val_test(df):\n",
    "    # split 70, 15, 15\n",
    "    train_index = int(len(df) * 0.7)\n",
    "    train_data = df[:train_index]\n",
    "    val_test_data = df[train_index:]\n",
    "    val_index = int(len(val_test_data) * 0.5)\n",
    "    val_data = val_test_data[:val_index]\n",
    "    test_data = val_test_data[val_index:]\n",
    "    print('Total data shape ', df.shape)\n",
    "    print('train shape ', train_data.shape)\n",
    "    print('validation shape ', val_data.shape)\n",
    "    print('test shape ', test_data.shape)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}sec')\n",
    "    \n",
    "def print_trace(name: str = ''):\n",
    "    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n",
    "    print(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e67a8c8",
   "metadata": {
    "papermill": {
     "duration": 0.030693,
     "end_time": "2022-01-23T02:26:30.479103",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.448410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Base Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9bcf32",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:30.573125Z",
     "iopub.status.busy": "2022-01-23T02:26:30.553308Z",
     "iopub.status.idle": "2022-01-23T02:26:30.575466Z",
     "shell.execute_reply": "2022-01-23T02:26:30.575064Z",
     "shell.execute_reply.started": "2022-01-19T11:20:47.920189Z"
    },
    "papermill": {
     "duration": 0.067985,
     "end_time": "2022-01-23T02:26:30.575573",
     "exception": false,
     "start_time": "2022-01-23T02:26:30.507588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataBlock(Enum):\n",
    "    TRAIN = 1\n",
    "    TEST = 2\n",
    "    BOTH = 3\n",
    "\n",
    "def load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', directory, f'stock_id={stock_id}'))\n",
    "\n",
    "def load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n",
    "    if block == DataBlock.TRAIN:\n",
    "        return load_stock_data(stock_id, f'{stem}_train.parquet')\n",
    "    elif block == DataBlock.TEST:\n",
    "        return load_stock_data(stock_id, f'{stem}_test.parquet')\n",
    "    else:\n",
    "        return pd.concat([\n",
    "            load_data(stock_id, stem, DataBlock.TRAIN),\n",
    "            load_data(stock_id, stem, DataBlock.TEST)\n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "def load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'book', block)\n",
    "\n",
    "def load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n",
    "    return load_data(stock_id, 'trade', block)\n",
    "\n",
    "def calc_wap1(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap2(df: pd.DataFrame) -> pd.Series:\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def log_return_df2(series: np.ndarray):\n",
    "    return np.log(series).diff(2)\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "def make_book_feature(stock_id, block = DataBlock.TRAIN, \n",
    "                      add_spread_features = False,\n",
    "                      add_statistics_features = False):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    book['wap1'] = calc_wap1(book)\n",
    "    book['wap2'] = calc_wap2(book)\n",
    "    book['log_return1'] = book.groupby(['time_id'], group_keys=False)['wap1'].apply(log_return)\n",
    "    book['log_return2'] = book.groupby(['time_id'], group_keys=False)['wap2'].apply(log_return)\n",
    "    book['log_return_ask1'] = book.groupby(['time_id'], group_keys=False)['ask_price1'].apply(log_return)\n",
    "    book['log_return_ask2'] = book.groupby(['time_id'], group_keys=False)['ask_price2'].apply(log_return)\n",
    "    book['log_return_bid1'] = book.groupby(['time_id'], group_keys=False)['bid_price1'].apply(log_return)\n",
    "    book['log_return_bid2'] = book.groupby(['time_id'], group_keys=False)['bid_price2'].apply(log_return)\n",
    "\n",
    "    if add_spread_features:\n",
    "        book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n",
    "        book['price_spread'] = (book['ask_price1'] - book['bid_price1']) / ((book['ask_price1'] + book['bid_price1']) / 2)\n",
    "        book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n",
    "        book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n",
    "        book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n",
    "        book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n",
    "\n",
    "    features = {\n",
    "        'wap1': [np.sum],\n",
    "        'wap2': [np.sum],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return_ask1': [realized_volatility],\n",
    "        'log_return_ask2': [realized_volatility],\n",
    "        'log_return_bid1': [realized_volatility],\n",
    "        'log_return_bid2': [realized_volatility],\n",
    "    }\n",
    "\n",
    "    if add_spread_features and add_statistics_features:\n",
    "        features = {\n",
    "            'seconds_in_bucket': ['count'],\n",
    "            'wap1': [np.sum, np.mean, np.std],\n",
    "            'wap2': [np.sum, np.mean, np.std],\n",
    "            'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "            'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "            'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "            'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "            'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n",
    "            'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n",
    "            'wap_balance': [np.sum, np.mean, np.std],\n",
    "            'price_spread':[np.sum, np.mean, np.std],\n",
    "            'bid_spread':[np.sum, np.mean, np.std],\n",
    "            'ask_spread':[np.sum, np.mean, np.std],\n",
    "            'total_volume':[np.sum, np.mean, np.std],\n",
    "            'volume_imbalance':[np.sum, np.mean, np.std]\n",
    "        }\n",
    "    elif add_spread_features and not add_statistics_features:\n",
    "        features = {\n",
    "            'seconds_in_bucket': ['count'],\n",
    "            'wap1': [np.sum],\n",
    "            'wap2': [np.sum],\n",
    "            'log_return1': [realized_volatility],\n",
    "            'log_return2': [realized_volatility],\n",
    "            'log_return_ask1': [np.sum, realized_volatility],\n",
    "            'log_return_ask2': [np.sum, realized_volatility],\n",
    "            'log_return_bid1': [np.sum, realized_volatility],\n",
    "            'log_return_bid2': [np.sum, realized_volatility],\n",
    "            'wap_balance': [np.sum],\n",
    "            'price_spread':[np.sum],\n",
    "            'bid_spread':[np.sum],\n",
    "            'ask_spread':[np.sum],\n",
    "            'total_volume':[np.sum],\n",
    "            'volume_imbalance':[np.sum]\n",
    "        }\n",
    "\n",
    "    \n",
    "    agg = book.groupby('time_id', group_keys=False).agg(features).reset_index(drop=False)\n",
    "    agg.columns = flatten_name('book', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "    \n",
    "    # for time in [450, 300, 150]:\n",
    "    #     d = book[book['seconds_in_bucket'] >= time].groupby('time_id', group_keys=False).agg(features).reset_index(drop=False)\n",
    "    #     d.columns = flatten_name(f'book_{time}', d.columns)\n",
    "    #     agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_trade_feature(stock_id, block = DataBlock.TRAIN):\n",
    "    trade = load_trade(stock_id, block)\n",
    "    trade['log_return'] = trade.groupby('time_id', group_keys=False)['price'].apply(log_return)\n",
    "\n",
    "    features = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':['count'],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.mean],\n",
    "    }\n",
    "\n",
    "    agg = trade.groupby('time_id', group_keys=False).agg(features).reset_index()\n",
    "    agg.columns = flatten_name('trade', agg.columns)\n",
    "    agg['stock_id'] = stock_id\n",
    "        \n",
    "    # for time in [450, 300, 150]:\n",
    "    #     d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n",
    "    #     d.columns = flatten_name(f'trade_{time}', d.columns)\n",
    "    #     agg = pd.merge(agg, d, on='time_id', how='left')\n",
    "    return agg\n",
    "\n",
    "def make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n",
    "    book = load_book(stock_id, block)\n",
    "\n",
    "    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n",
    "    time_ids = list(set(prices.index))\n",
    "\n",
    "    ticks = {}\n",
    "    for tid in time_ids:\n",
    "        try:\n",
    "            price_list = prices.loc[tid].values.flatten()\n",
    "            price_diff = sorted(np.diff(sorted(set(price_list))))\n",
    "            ticks[tid] = price_diff[0]\n",
    "        except Exception:\n",
    "            print_trace(f'tid={tid}')\n",
    "            ticks[tid] = np.nan\n",
    "        \n",
    "    dst = pd.DataFrame()\n",
    "    dst['time_id'] = np.unique(book['time_id'])\n",
    "    dst['stock_id'] = stock_id\n",
    "    dst['tick_size'] = dst['time_id'].map(ticks)\n",
    "\n",
    "    return dst\n",
    "\n",
    "def make_features(base, block, add_spread_features = False, add_statistics_features = False):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block, add_spread_features, add_statistics_features) for i in stock_ids)\n",
    "        book = pd.concat(books)\n",
    "\n",
    "    with timer('trades'):\n",
    "        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n",
    "        trade = pd.concat(trades)\n",
    "\n",
    "    with timer('extra features'):\n",
    "        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n",
    "        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_features_v2(base, block):\n",
    "    stock_ids = set(base['stock_id'])\n",
    "    with timer('books(v2)'):\n",
    "        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n",
    "        book_v2 = pd.concat(books)\n",
    "\n",
    "    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6d745fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book_df = load_book(0, DataBlock.TRAIN)\n",
    "# book_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1087ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# book_train_df = make_book_feature(0, DataBlock.TRAIN)\n",
    "# book_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c606639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_df = load_trade(0, DataBlock.TRAIN)\n",
    "# trade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33be8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_train_df = make_trade_feature(0, DataBlock.TRAIN)\n",
    "# trade_train_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e281cfe2",
   "metadata": {
    "papermill": {
     "duration": 0.028421,
     "end_time": "2022-01-23T02:26:41.480303",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.451882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Nearest-Neighbor Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aca7436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.555136Z",
     "iopub.status.busy": "2022-01-23T02:26:41.550462Z",
     "iopub.status.idle": "2022-01-23T02:26:41.557458Z",
     "shell.execute_reply": "2022-01-23T02:26:41.557058Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.104849Z"
    },
    "papermill": {
     "duration": 0.048663,
     "end_time": "2022-01-23T02:26:41.557564",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.508901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_NEIGHBORS_MAX = 1 # 80\n",
    "\n",
    "class Neighbors:\n",
    "    def __init__(self, \n",
    "                 name: str, \n",
    "                 pivot: pd.DataFrame, \n",
    "                 p: float, \n",
    "                 metric: str = 'minkowski', \n",
    "                 metric_params: Optional[Dict] = None, \n",
    "                 exclude_self: bool = False):\n",
    "        self.name = name\n",
    "        self.exclude_self = exclude_self\n",
    "        self.p = p\n",
    "        self.metric = metric\n",
    "        \n",
    "        if metric == 'random':\n",
    "            n_queries = len(pivot)\n",
    "            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n",
    "        else:\n",
    "            print('metric ', metric)\n",
    "            \n",
    "            nn = NearestNeighbors(\n",
    "                n_neighbors=N_NEIGHBORS_MAX, \n",
    "                p=p, \n",
    "                metric=metric, \n",
    "                metric_params=metric_params\n",
    "            )\n",
    "           \n",
    "            nn.fit(pivot)\n",
    "            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n",
    "\n",
    "        self.columns = self.index = self.feature_values = self.feature_col = None\n",
    "\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n",
    "        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n",
    "\n",
    "        start = 1 if self.exclude_self else 0\n",
    "\n",
    "        pivot_aggs = pd.DataFrame(\n",
    "            agg(self.feature_values[start:n,:,:], axis=0), \n",
    "            columns=self.columns, \n",
    "            index=self.index\n",
    "        )\n",
    "\n",
    "        dst = pivot_aggs.unstack().reset_index()\n",
    "        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n",
    "        return dst\n",
    "\n",
    "\n",
    "class TimeIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "        feature_pivot.head()\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n",
    "\n",
    "\n",
    "class StockIdNeighbors(Neighbors):\n",
    "    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n",
    "        \"\"\"stock-id based nearest neighbor features\"\"\"\n",
    "        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n",
    "        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n",
    "\n",
    "        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n",
    "\n",
    "        for i in range(N_NEIGHBORS_MAX):\n",
    "            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n",
    "\n",
    "        self.columns = list(feature_pivot.columns)\n",
    "        self.index = list(feature_pivot.index)\n",
    "        self.feature_values = feature_values\n",
    "        self.feature_col = feature_col\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972a076e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:41.619972Z",
     "iopub.status.busy": "2022-01-23T02:26:41.619012Z",
     "iopub.status.idle": "2022-01-23T02:26:41.982083Z",
     "shell.execute_reply": "2022-01-23T02:26:41.981564Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.127333Z"
    },
    "papermill": {
     "duration": 0.395558,
     "end_time": "2022-01-23T02:26:41.982243",
     "exception": false,
     "start_time": "2022-01-23T02:26:41.586685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_tau_features\n",
    "# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\n",
    "def add_tau_features(df_tau):\n",
    "    df_tau['trade.tau'] = np.sqrt(1 / df_tau['trade.seconds_in_bucket.count'])\n",
    "    df_tau['trade_150.tau'] = np.sqrt(1 / df_tau['trade_150.seconds_in_bucket.count'])\n",
    "    df_tau['book.tau'] = np.sqrt(1 / df_tau['book.seconds_in_bucket.count'])\n",
    "    df_tau['real_price'] = 0.01 / df_tau['tick_size']\n",
    "\n",
    "    return df_tau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be5a6240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-16T02:18:50.195022Z",
     "iopub.status.busy": "2022-01-16T02:18:50.1946Z",
     "iopub.status.idle": "2022-01-16T02:18:50.201136Z",
     "shell.execute_reply": "2022-01-16T02:18:50.199965Z",
     "shell.execute_reply.started": "2022-01-16T02:18:50.194964Z"
    },
    "papermill": {
     "duration": 0.030837,
     "end_time": "2022-01-23T02:26:42.050294",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.019457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Build Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64aead97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:26:42.123778Z",
     "iopub.status.busy": "2022-01-23T02:26:42.122544Z",
     "iopub.status.idle": "2022-01-23T02:33:32.953387Z",
     "shell.execute_reply": "2022-01-23T02:33:32.953798Z",
     "shell.execute_reply.started": "2022-01-19T11:20:58.499414Z"
    },
    "papermill": {
     "duration": 410.874751,
     "end_time": "2022-01-23T02:33:32.953989",
     "exception": false,
     "start_time": "2022-01-23T02:26:42.079238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build_nearest_neighbors\n",
    "def build_nearest_neighbors(df_nn, \n",
    "    use_price_nn_features, \n",
    "    use_volume_nn_features, \n",
    "    use_size_nn_features, \n",
    "    use_random_nn_features):\n",
    "    time_id_neighbors: List[Neighbors] = []\n",
    "    stock_id_neighbors: List[Neighbors] = []\n",
    "\n",
    "    with timer('knn fit'):\n",
    "        df_pv = df_nn[['stock_id', 'time_id']].copy()\n",
    "        df_pv['price'] = 0.01 / df_nn['tick_size']\n",
    "        df_pv['vol'] = df_nn['book.log_return1.realized_volatility']\n",
    "        df_pv['trade.tau'] = df_nn['trade.tau']\n",
    "        df_pv['trade.size.sum'] = df_nn['book.total_volume.sum']\n",
    "\n",
    "        print('USE_PRICE_NN_FEATURES ', use_price_nn_features)\n",
    "        if use_price_nn_features:\n",
    "            pivot = df_pv.pivot('time_id', 'stock_id', 'price')\n",
    "            pivot = pivot.fillna(pivot.mean())\n",
    "            pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    'time_price_c', \n",
    "                    pivot, \n",
    "                    p=2, \n",
    "                    metric='canberra', \n",
    "                    exclude_self=True\n",
    "                )\n",
    "            )\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    'time_price_m', \n",
    "                    pivot, \n",
    "                    p=2, \n",
    "                    metric='mahalanobis',\n",
    "                    metric_params={'VI':np.linalg.inv(np.cov(pivot.values.T))}\n",
    "                )\n",
    "            )\n",
    "            stock_id_neighbors.append(\n",
    "                StockIdNeighbors(\n",
    "                    'stock_price_l1', \n",
    "                    minmax_scale(pivot.transpose()), \n",
    "                    p=1, \n",
    "                    exclude_self=True)\n",
    "            )\n",
    "\n",
    "        print('USE_VOL_NN_FEATURES ', use_volume_nn_features)\n",
    "        if use_volume_nn_features:\n",
    "            pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "            pivot = pivot.fillna(pivot.mean())\n",
    "            pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors('time_vol_l1', pivot, p=1)\n",
    "            )\n",
    "            stock_id_neighbors.append(\n",
    "                StockIdNeighbors(\n",
    "                    'stock_vol_l1', \n",
    "                    minmax_scale(pivot.transpose()), \n",
    "                    p=1, \n",
    "                    exclude_self=True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print('USE_SIZE_NN_FEATURES ', use_size_nn_features)\n",
    "        if use_size_nn_features:\n",
    "            pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n",
    "            pivot = pivot.fillna(pivot.mean())\n",
    "            pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    'time_size_m', \n",
    "                    pivot, \n",
    "                    p=2, \n",
    "                    metric='mahalanobis', \n",
    "                    # metric_params={'V':np.cov(pivot.values.T)}\n",
    "                    metric_params={'VI':np.linalg.inv(np.cov(pivot.values.T))}\n",
    "                )\n",
    "            )\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    'time_size_c', \n",
    "                    pivot, \n",
    "                    p=2, \n",
    "                    metric='canberra'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        print('USE_RANDOM_NN_FEATURES ', use_random_nn_features)\n",
    "        if use_random_nn_features:\n",
    "            pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n",
    "            pivot = pivot.fillna(pivot.mean())\n",
    "            pivot = pd.DataFrame(minmax_scale(pivot))\n",
    "\n",
    "            time_id_neighbors.append(\n",
    "                TimeIdNeighbors(\n",
    "                    'time_random', \n",
    "                    pivot, \n",
    "                    p=2, \n",
    "                    metric='random'\n",
    "                )\n",
    "            )\n",
    "            stock_id_neighbors.append(\n",
    "                StockIdNeighbors(\n",
    "                    'stock_random', \n",
    "                    pivot.transpose(), \n",
    "                    p=2,\n",
    "                    metric='random')\n",
    "            )\n",
    "            \n",
    "    return time_id_neighbors, stock_id_neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27029360",
   "metadata": {
    "papermill": {
     "duration": 0.028479,
     "end_time": "2022-01-23T02:33:33.011086",
     "exception": false,
     "start_time": "2022-01-23T02:33:32.982607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check Neighbor Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65fd5a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.073803Z",
     "iopub.status.busy": "2022-01-23T02:33:33.073080Z",
     "iopub.status.idle": "2022-01-23T02:33:33.075920Z",
     "shell.execute_reply": "2022-01-23T02:33:33.075471Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.548287Z"
    },
    "papermill": {
     "duration": 0.035942,
     "end_time": "2022-01-23T02:33:33.076032",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.040090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate_rank_correraltion\n",
    "def calculate_rank_correraltion(neighbors, top_n=5):\n",
    "    if not neighbors:\n",
    "        return\n",
    "    neighbor_indices = pd.DataFrame()\n",
    "    for n in neighbors:\n",
    "        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n",
    "\n",
    "    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5123638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.138908Z",
     "iopub.status.busy": "2022-01-23T02:33:33.138122Z",
     "iopub.status.idle": "2022-01-23T02:33:33.192860Z",
     "shell.execute_reply": "2022-01-23T02:33:33.192422Z",
     "shell.execute_reply.started": "2022-01-19T11:27:55.555683Z"
    },
    "papermill": {
     "duration": 0.08837,
     "end_time": "2022-01-23T02:33:33.192975",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.104605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display_neighbors\n",
    "def display_neighbors(df_neighbor, neighbors_to_display, column_name, number_of_neighbor):\n",
    "    ids = np.array(sorted(df_neighbor[column_name].unique()))\n",
    "    for neighbor in neighbors_to_display:\n",
    "        print(neighbor)\n",
    "        display(\n",
    "            pd.DataFrame(\n",
    "                ids[neighbor.neighbors[:,:number_of_neighbor]], \n",
    "                index=pd.Index(ids, name=column_name), \n",
    "                # ALERT: NOTE value was 10 in range and was updated to 2\n",
    "                columns=[f'top_{i+1}' for i in range(number_of_neighbor)] #10\n",
    "            ).iloc[1:6]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68bfd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_neighbors(df, time_id_neighbors, 'time_id', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7276b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_neighbors(df, stock_id_neighbors, 'stock_id', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2589b60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.263643Z",
     "iopub.status.busy": "2022-01-23T02:33:33.262935Z",
     "iopub.status.idle": "2022-01-23T02:33:33.276604Z",
     "shell.execute_reply": "2022-01-23T02:33:33.276173Z",
     "shell.execute_reply.started": "2022-01-19T11:39:40.610534Z"
    },
    "papermill": {
     "duration": 0.051151,
     "end_time": "2022-01-23T02:33:33.276704",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.225553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# stock_ids = np.array(sorted(df['stock_id'].unique()))\n",
    "# for neighbor in stock_id_neighbors:\n",
    "#     print(neighbor)\n",
    "#     display(\n",
    "#         pd.DataFrame(\n",
    "#             stock_ids[neighbor.neighbors[:,:10]], \n",
    "#             index=pd.Index(stock_ids, name='stock_id'), \n",
    "#             # NOTE: range was 10,\n",
    "#             columns=[f'top_{i+1}' for i in range(10)] #10\n",
    "#         ).loc[0] #64\n",
    "#     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10b5471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:33.348457Z",
     "iopub.status.busy": "2022-01-23T02:33:33.347681Z",
     "iopub.status.idle": "2022-01-23T02:33:33.901994Z",
     "shell.execute_reply": "2022-01-23T02:33:33.902453Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.542166Z"
    },
    "papermill": {
     "duration": 0.591893,
     "end_time": "2022-01-23T02:33:33.902600",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.310707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate_rank_correraltion(time_id_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35df3e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.038369Z",
     "iopub.status.busy": "2022-01-23T02:33:34.037754Z",
     "iopub.status.idle": "2022-01-23T02:33:34.275994Z",
     "shell.execute_reply": "2022-01-23T02:33:34.276373Z",
     "shell.execute_reply.started": "2022-01-18T14:13:43.949648Z"
    },
    "papermill": {
     "duration": 0.313195,
     "end_time": "2022-01-23T02:33:34.276516",
     "exception": false,
     "start_time": "2022-01-23T02:33:33.963321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate_rank_correraltion(stock_id_neighbors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac11498c",
   "metadata": {
    "papermill": {
     "duration": 0.035477,
     "end_time": "2022-01-23T02:33:34.347529",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.312052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Aggregate Features With Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec957b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:34.427036Z",
     "iopub.status.busy": "2022-01-23T02:33:34.426197Z",
     "iopub.status.idle": "2022-01-23T02:33:36.057149Z",
     "shell.execute_reply": "2022-01-23T02:33:36.056625Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.189634Z"
    },
    "papermill": {
     "duration": 1.674163,
     "end_time": "2022-01-23T02:33:36.057283",
     "exception": false,
     "start_time": "2022-01-23T02:33:34.383120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aggregate_features_with_neighbors\n",
    "# features with large changes over time are converted to relative ranks within time-id\n",
    "def aggregate_features_with_neighbors(df_agg):\n",
    "    df_agg['trade.order_count.mean'] = df_agg.groupby('time_id', group_keys=False)['trade.order_count.mean'].rank()\n",
    "    df_agg['book.total_volume.sum']  = df_agg.groupby('time_id', group_keys=False)['book.total_volume.sum'].rank()\n",
    "    df_agg['book.total_volume.mean'] = df_agg.groupby('time_id', group_keys=False)['book.total_volume.mean'].rank()\n",
    "    df_agg['book.total_volume.std']  = df_agg.groupby('time_id')['book.total_volume.std'].rank()\n",
    "\n",
    "    df_agg['trade.tau'] = df_agg.groupby('time_id', group_keys=False)['trade.tau'].rank()\n",
    "\n",
    "    for dt in [150, 300, 450]:\n",
    "        df_agg[f'book_{dt}.total_volume.sum']  = df_agg.groupby('time_id', group_keys=False)[f'book_{dt}.total_volume.sum'].rank()\n",
    "        df_agg[f'book_{dt}.total_volume.mean'] = df_agg.groupby('time_id', group_keys=False)[f'book_{dt}.total_volume.mean'].rank()\n",
    "        df_agg[f'book_{dt}.total_volume.std']  = df_agg.groupby('time_id', group_keys=False)[f'book_{dt}.total_volume.std'].rank()\n",
    "        df_agg[f'trade_{dt}.order_count.mean'] = df_agg.groupby('time_id', group_keys=False)[f'trade_{dt}.order_count.mean'].rank()\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6efc7d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:33:36.151666Z",
     "iopub.status.busy": "2022-01-23T02:33:36.150004Z",
     "iopub.status.idle": "2022-01-23T02:33:36.152354Z",
     "shell.execute_reply": "2022-01-23T02:33:36.152748Z",
     "shell.execute_reply.started": "2022-01-18T14:13:44.199422Z"
    },
    "papermill": {
     "duration": 0.059468,
     "end_time": "2022-01-23T02:33:36.152899",
     "exception": false,
     "start_time": "2022-01-23T02:33:36.093431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make_nearest_neighbor_feature\n",
    "def make_nearest_neighbor_feature(df_nn: pd.DataFrame, time_id_neighbors, stock_id_neighbors) -> pd.DataFrame:\n",
    "    df_nnf = df_nn.copy()\n",
    "\n",
    "    feature_cols_stock = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade_150.tau': [np.mean],\n",
    "        'book.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "    }\n",
    "    \n",
    "    feature_cols = {\n",
    "        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n",
    "        'real_price': [np.max, np.mean, np.min],\n",
    "        'trade.seconds_in_bucket.count': [np.mean],\n",
    "        'trade.tau': [np.mean],\n",
    "        'trade.size.sum': [np.mean],\n",
    "        'book.seconds_in_bucket.count': [np.mean],\n",
    "        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n",
    "        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n",
    "    }\n",
    "\n",
    "    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n",
    "    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n",
    "    stock_id_neighbor_sizes = [10, 20, 40]\n",
    "\n",
    "    ndf: Optional[pd.DataFrame] = None\n",
    "\n",
    "    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n",
    "        if ndf is None:\n",
    "            return dst\n",
    "        else:\n",
    "            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n",
    "            return ndf\n",
    "\n",
    "    # neighbor stock_id\n",
    "    for feature_col in feature_cols_stock.keys():\n",
    "        try:\n",
    "            if feature_col not in df_nnf.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            if not stock_id_neighbors:\n",
    "                continue\n",
    "\n",
    "            for nn in stock_id_neighbors:\n",
    "                nn.rearrange_feature_values(df_nnf, feature_col)\n",
    "\n",
    "            for agg in feature_cols_stock[feature_col]:\n",
    "                for n in stock_id_neighbor_sizes:\n",
    "                    try:\n",
    "                        for nn in stock_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('stock-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('stock-id nn')\n",
    "            pass\n",
    "\n",
    "    if ndf is not None:\n",
    "        df_nnf = pd.merge(df_nnf, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "    ndf = None\n",
    "\n",
    "    # neighbor time_id\n",
    "    for feature_col in feature_cols.keys():\n",
    "        try:\n",
    "            if feature_col == 'real_price':\n",
    "                continue\n",
    "            if feature_col not in df_nnf.columns:\n",
    "                print(f\"column {feature_col} is skipped\")\n",
    "                continue\n",
    "\n",
    "            for nn in time_id_neighbors:\n",
    "                nn.rearrange_feature_values(df_nnf, feature_col)\n",
    "\n",
    "            if 'volatility' in feature_col:\n",
    "                time_id_ns = time_id_neigbor_sizes_vol\n",
    "            else:\n",
    "                time_id_ns = time_id_neigbor_sizes\n",
    "\n",
    "            for agg in feature_cols[feature_col]:\n",
    "                for n in time_id_ns:\n",
    "                    try:\n",
    "                        for nn in time_id_neighbors:\n",
    "                            dst = nn.make_nn_feature(n, agg)\n",
    "                            ndf = _add_ndf(ndf, dst)\n",
    "                    except Exception:\n",
    "                        print_trace('time-id nn')\n",
    "                        pass\n",
    "        except Exception:\n",
    "            print_trace('time-id nn')\n",
    "\n",
    "    if ndf is not None:\n",
    "        df_nnf = pd.merge(df_nnf, ndf, on=['time_id', 'stock_id'], how='left')\n",
    "\n",
    "    # features further derived from nearest neighbor features\n",
    "    try:\n",
    "        for sz in time_id_neigbor_sizes:\n",
    "            denominator = f\"real_price_nn{sz}_time_price_c\"\n",
    "\n",
    "            df_nnf[f'real_price_rankmin_{sz}']  = df_nnf['real_price'] / df2[f\"{denominator}_amin\"]\n",
    "            df_nnf[f'real_price_rankmax_{sz}']  = df_nnf['real_price'] / df2[f\"{denominator}_amax\"]\n",
    "            df_nnf[f'real_price_rankmean_{sz}'] = df_nnf['real_price'] / df2[f\"{denominator}_mean\"]\n",
    "\n",
    "        for sz in time_id_neigbor_sizes_vol:\n",
    "            denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n",
    "\n",
    "            df_nnf[f'vol_rankmin_{sz}'] = \\\n",
    "                df_nnf['book.log_return1.realized_volatility'] / df_nnf[f\"{denominator}_amin\"]\n",
    "            df_nnf[f'vol_rankmax_{sz}'] = \\\n",
    "                df_nnf['book.log_return1.realized_volatility'] / df_nnf[f\"{denominator}_amax\"]\n",
    "\n",
    "        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n",
    "        for c in price_cols:\n",
    "            del df_nnf[c]\n",
    "\n",
    "        for sz in time_id_neigbor_sizes_vol:\n",
    "            tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n",
    "            df_nnf[f'{tgt}_rank'] = df_nnf.groupby('time_id', group_keys=False)[tgt].rank()\n",
    "    except Exception:\n",
    "        print_trace('nn features')\n",
    "\n",
    "    return df_nnf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "049e3ff0",
   "metadata": {
    "papermill": {
     "duration": 0.037563,
     "end_time": "2022-01-23T02:34:52.038355",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.000792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Misc Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b672b203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.076612Z",
     "iopub.status.busy": "2022-01-23T02:34:53.075859Z",
     "iopub.status.idle": "2022-01-23T02:34:53.335135Z",
     "shell.execute_reply": "2022-01-23T02:34:53.334610Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.290787Z"
    },
    "papermill": {
     "duration": 1.258742,
     "end_time": "2022-01-23T02:34:53.335258",
     "exception": false,
     "start_time": "2022-01-23T02:34:52.076516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# skew correction for NN\n",
    "def skew_correction_for_nn(df_skew):\n",
    "    cols_to_log = [\n",
    "        'trade.size.sum',\n",
    "        'trade_150.size.sum',\n",
    "        'trade_300.size.sum',\n",
    "        'trade_450.size.sum',\n",
    "        'volume_imbalance'\n",
    "    ]\n",
    "    for c in df_skew.columns:\n",
    "        for check in cols_to_log:\n",
    "            try:\n",
    "                if check in c:\n",
    "                    df_skew[c] = np.log(df_skew[c]+1)\n",
    "                    break\n",
    "            except Exception:\n",
    "                print_trace('log1p')\n",
    "\n",
    "    return df_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76b80e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:53.415634Z",
     "iopub.status.busy": "2022-01-23T02:34:53.414883Z",
     "iopub.status.idle": "2022-01-23T02:34:54.757020Z",
     "shell.execute_reply": "2022-01-23T02:34:54.756480Z",
     "shell.execute_reply.started": "2022-01-15T04:54:06.724354Z"
    },
    "papermill": {
     "duration": 1.384579,
     "end_time": "2022-01-23T02:34:54.757155",
     "exception": false,
     "start_time": "2022-01-23T02:34:53.372576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rolling average of RV for similar trading volume\n",
    "def rolling_average_of_rv_for_similar_trading_volume(df_ra):\n",
    "    try:\n",
    "        df_ra.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n",
    "        df_ra.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        roll_target = 'book.log_return1.realized_volatility'\n",
    "\n",
    "        for window_size in [3, 10]:\n",
    "            df_ra[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n",
    "                df_ra.groupby('stock_id', group_keys=False)[roll_target].rolling(window_size, center=True, min_periods=1) \\\n",
    "                                                    .mean() \\\n",
    "                                                    .reset_index() \\\n",
    "                                                    .sort_values(by=['level_1'])[roll_target].values\n",
    "    except Exception:\n",
    "        print_trace('mean RV')\n",
    "\n",
    "    return df_ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ac17ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:54.865738Z",
     "iopub.status.busy": "2022-01-23T02:34:54.859008Z",
     "iopub.status.idle": "2022-01-23T02:34:57.630440Z",
     "shell.execute_reply": "2022-01-23T02:34:57.631709Z",
     "shell.execute_reply.started": "2022-01-15T04:54:08.318718Z"
    },
    "papermill": {
     "duration": 2.836215,
     "end_time": "2022-01-23T02:34:57.631962",
     "exception": false,
     "start_time": "2022-01-23T02:34:54.795747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # stock-id embedding (helps little)\n",
    "# try:\n",
    "#     lda_n = 3\n",
    "#     lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n",
    "\n",
    "#     stock_id_emb = pd.DataFrame(\n",
    "#         lda.fit_transform(pivot.transpose()), \n",
    "#         index=df_pv.pivot('time_id', 'stock_id', 'vol').columns\n",
    "#     )\n",
    "\n",
    "#     for i in range(lda_n):\n",
    "#         df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\n",
    "# except Exception:\n",
    "#     print_trace('LDA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f898c5d1",
   "metadata": {
    "papermill": {
     "duration": 0.037142,
     "end_time": "2022-01-23T02:34:59.516263",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.479121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reverse Engineering time-id Order & Make CV Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1306bfda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.616772Z",
     "iopub.status.busy": "2022-01-23T02:34:59.615063Z",
     "iopub.status.idle": "2022-01-23T02:34:59.617414Z",
     "shell.execute_reply": "2022-01-23T02:34:59.617868Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.7715Z"
    },
    "papermill": {
     "duration": 0.063706,
     "end_time": "2022-01-23T02:34:59.618003",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.554297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reverse engineering time-id order\n",
    "%matplotlib inline\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    e = time.time() - s\n",
    "    print(f\"[{name}] {e:.3f}sec\")\n",
    "    \n",
    "def calc_price2(df):\n",
    "    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n",
    "    return 0.01 / tick\n",
    "\n",
    "def calc_prices(r):\n",
    "    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n",
    "    df = df.set_index('time_id')\n",
    "    df = df.groupby(level='time_id', group_keys=False).apply(calc_price2).to_frame('price').reset_index()\n",
    "    df['stock_id'] = r.stock_id\n",
    "    return df\n",
    "\n",
    "def sort_manifold(df, clf):\n",
    "    df_ = df.set_index('time_id')\n",
    "    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n",
    "\n",
    "    X_compoents = clf.fit_transform(df_)\n",
    "\n",
    "    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n",
    "    return np.argsort(X_compoents[:, 0]), X_compoents\n",
    "\n",
    "def reconstruct_time_id_order():\n",
    "    with timer('load files'):\n",
    "        book_path = DATA_DIR + '/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet'\n",
    "        print('book path ', book_path)\n",
    "        df_files = pd.DataFrame(\n",
    "            {'book_path': glob.glob(book_path)}) \\\n",
    "            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n",
    "\n",
    "    with timer('calc prices'):\n",
    "        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n",
    "        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n",
    "        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n",
    "        df_prices = df_prices.reset_index(drop=False)\n",
    "\n",
    "    with timer('t-SNE(400) -> 50'):\n",
    "        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n",
    "        order, X_compoents = sort_manifold(df_prices, clf)\n",
    "\n",
    "        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n",
    "        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n",
    "            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n",
    "\n",
    "    # AMZN\n",
    "    # plt.plot(df_ordered['stock_id=61'])\n",
    "    \n",
    "    return df_ordered[['time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c3ad6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:34:59.705424Z",
     "iopub.status.busy": "2022-01-23T02:34:59.704651Z",
     "iopub.status.idle": "2022-01-23T02:35:01.635920Z",
     "shell.execute_reply": "2022-01-23T02:35:01.636972Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.801275Z"
    },
    "papermill": {
     "duration": 1.981013,
     "end_time": "2022-01-23T02:35:01.637181",
     "exception": false,
     "start_time": "2022-01-23T02:34:59.656168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_time_id_order\n",
    "def add_time_id_order(df_tid):\n",
    "    timeid_order = reconstruct_time_id_order()\n",
    "    timeid_order['time_id_order'] = np.arange(len(timeid_order))\n",
    "    df_tid['time_id_order'] = df_tid['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n",
    "    df_tid = df_tid.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n",
    "    df_tid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_tid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b888bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chek_null_columns\n",
    "def chek_null_columns(X):\n",
    "    xp = X.isna().any()\n",
    "    xp_null = xp.loc[lambda x : x == True]\n",
    "    nan_columns = list(xp_null.index)\n",
    "    print('Null columns ', nan_columns)\n",
    "    # X = X.drop(columns=nan_columns)\n",
    "    # return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ee88bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_time_id_ordered\n",
    "def plot_time_id_ordered(stock_id, df, first_n_records = None):\n",
    "    df_train_per_stock = df[df['stock_id'] == stock_id]\n",
    "    if first_n_records:\n",
    "        df_train_per_stock = df_train_per_stock[0: first_n_records]\n",
    "    print('df_train_per_stock.shape',df_train_per_stock.shape)\n",
    "    plt.plot(range(len(df_train_per_stock)), df_train_per_stock['target'])\n",
    "    plt.title('Time Id ordered plot of target')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Realized volatility')\n",
    "    plt.title('Reealized volatility for stock ' + str(stock_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e68994df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_time_id_ordered_plot(0, df_train, 36*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5adb5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_time_id_ordered_plot(0, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31e0e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1e3826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modal results operations \n",
    "def get_model_results_df():\n",
    "    return pd.DataFrame(model_results)\n",
    "\n",
    "def reset_model_results():\n",
    "    model_results = []\n",
    "\n",
    "def add_model_result(model_name: str, y_true, y_pred, isDart: bool, feature: str, time_taken: datetime):\n",
    "    if y_true is None:\n",
    "        raise ValueError(\"y_true is None\")\n",
    "    \n",
    "    if y_pred is None:\n",
    "        raise ValueError(\"y_pred is None\")\n",
    "\n",
    "    if isDart:\n",
    "        print('using dart metrics')\n",
    "        mae_value = mae(y_true, y_pred)\n",
    "        rmse_value = rmse(y_true, y_pred)\n",
    "        mse_value = mse(y_true, y_pred)\n",
    "    else:\n",
    "        print('using sklearn metrics')\n",
    "        mse_value = mean_squared_error(y_true, y_pred)\n",
    "        rmse_value = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        mae_value = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    model_result_existing = [m for m in model_results if \n",
    "                             m['model_name'].lower() == model_name.lower() and \n",
    "                             m['feature'].lower() == feature.lower()]\n",
    "    if model_result_existing: \n",
    "        print('value already exists in model results. So updating it')\n",
    "        for model in model_results:\n",
    "            if model['model_name'] == model_name:\n",
    "                model['mse'] = mse_value\n",
    "                model['mae'] = mae_value\n",
    "                model['rmse'] = rmse_value\n",
    "                model['added_date'] = datetime.now()\n",
    "                model['time_taken'] = time_taken\n",
    "    else:\n",
    "        print('adding new model results in')\n",
    "        model_results.append({'model_name': model_name, \n",
    "                              'mse': mse_value, \n",
    "                              'rmse': rmse_value, \n",
    "                              'mae': mae_value, \n",
    "                              'added_date': datetime.now(),\n",
    "                              'feature': feature\n",
    "                              })\n",
    "\n",
    "    return model_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "020eecf0",
   "metadata": {
    "papermill": {
     "duration": 0.067715,
     "end_time": "2022-01-23T02:35:01.777174",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.709459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LightGBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36f6f703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:01.921148Z",
     "iopub.status.busy": "2022-01-23T02:35:01.920367Z",
     "iopub.status.idle": "2022-01-23T02:35:01.933964Z",
     "shell.execute_reply": "2022-01-23T02:35:01.934928Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.902446Z"
    },
    "papermill": {
     "duration": 0.091675,
     "end_time": "2022-01-23T02:35:01.935102",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.843427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# light gbm\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n",
    "\n",
    "def feval_RMSPE(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n",
    "\n",
    "# from: https://blog.amedama.jp/entry/lightgbm-cv-feature-importance\n",
    "def plot_importance(cvbooster, figsize=(10, 10)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # order by average importance across folds\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # plot top-n\n",
    "    PLOT_TOP_N = 50\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "def get_X(df_src):\n",
    "    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n",
    "    return df_src[cols]\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "\n",
    "        features = list(self.models[0].feature_name())\n",
    "\n",
    "        for m in self.models[1:]:\n",
    "            assert features == list(m.feature_name())\n",
    "\n",
    "    def predict(self, x):\n",
    "        predicted = np.zeros((len(x), len(self.models)))\n",
    "\n",
    "        for i, m in enumerate(self.models):\n",
    "            w = self.weights[i] if self.weights is not None else 1\n",
    "            predicted[:, i] = w * m.predict(x)\n",
    "\n",
    "        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n",
    "        return np.sum(predicted, axis=1) / ttl\n",
    "\n",
    "    def feature_name(self) -> List[str]:\n",
    "        return self.models[0].feature_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0afb222a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T02:35:02.048565Z",
     "iopub.status.busy": "2022-01-23T02:35:02.044618Z",
     "iopub.status.idle": "2022-01-23T03:35:55.320275Z",
     "shell.execute_reply": "2022-01-23T03:35:55.319782Z",
     "shell.execute_reply.started": "2022-01-15T04:54:14.922483Z"
    },
    "papermill": {
     "duration": 3653.32245,
     "end_time": "2022-01-23T03:35:55.320410",
     "exception": false,
     "start_time": "2022-01-23T02:35:01.997960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_results_from_light_gbm\n",
    "def add_results_from_light_gbm(X_train_lgbm, y_train_lgbm, X_test_lgbm, y_test_lgbm, feature, lr=0.3):\n",
    "    params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0,\n",
    "    'metric': '',\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 5,\n",
    "    'min_data_in_leaf': 1000,\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 128,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'learning_rate': lr\n",
    "    }\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    ds = lgb.Dataset(X_train_lgbm, y_train_lgbm, weight=1/np.power(y_train_lgbm, 2))\n",
    "\n",
    "    ret = lgb.cv(params, ds, num_boost_round=8000, \n",
    "                    feval=feval_RMSPE, \n",
    "                    stratified=False, \n",
    "                    return_cvbooster=True, \n",
    "                    verbose_eval=20,\n",
    "                    early_stopping_rounds=int(40*0.1/lr))\n",
    "\n",
    "    # print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n",
    "\n",
    "    best_iteration = len(ret['RMSPE-mean'])\n",
    "\n",
    "    # print('boosters length ', len(ret['cvbooster'].boosters))\n",
    "\n",
    "    best_mae = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for i in range(len(ret['cvbooster'].boosters)):\n",
    "        y_pred = ret['cvbooster'].boosters[i].predict(X_test_lgbm, num_iteration=best_iteration)\n",
    "        print('y_pred here ', y_pred)\n",
    "        mae_value = mean_absolute_error(y_test_lgbm, y_pred)\n",
    "        print('mae value ', mae_value)\n",
    "        if best_mae == None:\n",
    "            best_mae = mae_value\n",
    "\n",
    "        if mae_value < best_mae:\n",
    "            print('updating best mae value')\n",
    "            best_mae = mae_value\n",
    "            best_y_pred = y_pred\n",
    "        \n",
    "    time_taken = datetime.now() - start_time\n",
    "    add_model_result('LightGBM', y_test_lgbm, best_y_pred, False, feature, time_taken)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fbd4e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_importance(ret['cvbooster'], figsize=(10, 20))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb015e20",
   "metadata": {
    "papermill": {
     "duration": 0.057064,
     "end_time": "2022-01-23T03:35:55.434906",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.377842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dce13e5",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:55.569557Z",
     "iopub.status.busy": "2022-01-23T03:35:55.558558Z",
     "iopub.status.idle": "2022-01-23T03:35:56.706846Z",
     "shell.execute_reply": "2022-01-23T03:35:56.705908Z",
     "shell.execute_reply.started": "2022-01-15T04:57:16.2193Z"
    },
    "papermill": {
     "duration": 1.211778,
     "end_time": "2022-01-23T03:35:56.706992",
     "exception": false,
     "start_time": "2022-01-23T03:35:55.495214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN Training\n",
    "\n",
    "NUM_WORKERS = 0 #4\n",
    "\n",
    "null_check_cols = [\n",
    "    'book.log_return1.realized_volatility',\n",
    "    'book_150.log_return1.realized_volatility',\n",
    "    'book_300.log_return1.realized_volatility',\n",
    "    'book_450.log_return1.realized_volatility',\n",
    "    'trade.log_return.realized_volatility',\n",
    "    'trade_150.log_return.realized_volatility',\n",
    "    'trade_300.log_return.realized_volatility',\n",
    "    'trade_450.log_return.realized_volatility'\n",
    "]\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def rmspe_metric(y_true, y_pred):\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_loss(y_true, y_pred):\n",
    "    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss_Tabnet(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx]\n",
    "        else:\n",
    "            return self.x_num[idx], self.y[idx]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num):\n",
    "        x = self.sequence(x_num)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num):\n",
    "        x = self.expand(x_num)\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "        x = self.conv1(x)\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "# def preprocess_nn(\n",
    "#         X: pd.DataFrame,\n",
    "#         scaler: Optional[StandardScaler] = None,\n",
    "#         scaler_type: str = 'standard',\n",
    "#         n_pca: int = -1,\n",
    "#         na_cols: bool = True):\n",
    "#     if na_cols:\n",
    "#         #for c in X.columns:\n",
    "#         for c in null_check_cols:\n",
    "#             if c in X.columns:\n",
    "#                 X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "\n",
    "#     cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n",
    "#     num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "#     X_num = X[num_cols].values.astype(np.float32)\n",
    "#     X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "#     def _pca(X_num_):\n",
    "#         if n_pca > 0:\n",
    "#             pca = PCA(n_components=n_pca, random_state=0)\n",
    "#             return pca.fit_transform(X_num)\n",
    "#         return X_num\n",
    "\n",
    "#     if scaler is None:\n",
    "#         scaler = StandardScaler()\n",
    "#         X_num = scaler.fit_transform(X_num)\n",
    "#         X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "#         return _pca(X_num), X_cat, cat_cols, scaler\n",
    "#     else:\n",
    "#         X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "#         X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "#         return _pca(X_num), X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    "\n",
    "    for x_num, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        loss = rmspe_loss(y, model(x_num))\n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num)\n",
    "\n",
    "            loss = rmspe_loss(y, output)\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        metric = rmspe_metric(final_targets, final_outputs)\n",
    "    except:\n",
    "        metric = None\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, metric\n",
    "\n",
    "def predict_nn(X_df: pd.DataFrame,\n",
    "               model: Union[List[MLP], MLP],\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "\n",
    "    evaluation_dataset = TabularDataset(X_df.values, None)\n",
    "    evaluation_data_loader = torch.utils.data.DataLoader(evaluation_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=NUM_WORKERS)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num in tqdm(evaluation_data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            \n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def train_nn(\n",
    "             X_train_df,\n",
    "             y_train_df,\n",
    "             X_val_df,\n",
    "             y_val_df,\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             model_type: str = 'mlp',\n",
    "             mlp_dropout: float = 0.0,\n",
    "             mlp_hidden: int = 64,\n",
    "             mlp_bn: bool = False,\n",
    "             cnn_hidden: int = 64,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             scaler_type: str = 'standard',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             n_pca: int = -1,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             na_cols: bool = True,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "\n",
    "    cur_batch = batch_size\n",
    "    best_loss = 1e10\n",
    "    best_prediction = None\n",
    "    train_dataset = TabularDataset(X_train_df.values, y_train_df.values)\n",
    "    valid_dataset = TabularDataset(X_val_df.values, y_val_df.values)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                num_workers=NUM_WORKERS)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                num_workers=NUM_WORKERS)\n",
    "\n",
    "    if model_type == 'mlp':\n",
    "        model = MLP(X_train_df.shape[1],\n",
    "                    dropout=mlp_dropout, \n",
    "                    hidden=mlp_hidden, \n",
    "                    bn=mlp_bn)\n",
    "    elif model_type == 'cnn':\n",
    "        model = CNN(X_train_df.shape[1],\n",
    "                    hidden_size=cnn_hidden,\n",
    "                    emb_dim=emb_dim,\n",
    "                    dropout_cat=dropout_emb,\n",
    "                    channel_1=cnn_channel1,\n",
    "                    channel_2=cnn_channel2,\n",
    "                    channel_3=cnn_channel3,\n",
    "                    two_stage=False,\n",
    "                    kernel1=cnn_kernel1,\n",
    "                    celu=cnn_celu,\n",
    "                    dropout_top=cnn_dropout,\n",
    "                    dropout_mid=cnn_dropout,\n",
    "                    dropout_bottom=cnn_dropout,\n",
    "                    weight_norm=cnn_weight_norm,\n",
    "                    leaky_relu=cnn_leaky_relu)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    model = model.to(device)\n",
    "\n",
    "    if optimizer_type == 'adamw':\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'adam':\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    scheduler = epoch_scheduler = None\n",
    "    if scheduler_type == 'onecycle':\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                        max_lr=max_lr, epochs=epochs,\n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "    elif scheduler_type == 'reduce':\n",
    "        epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                        mode='min',\n",
    "                                                                        min_lr=1e-7,\n",
    "                                                                        patience=patience,\n",
    "                                                                        verbose=True,\n",
    "                                                                        factor=factor)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "            cur_batch = cur_batch * 2\n",
    "            print(f'batch: {cur_batch}')\n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                        batch_size=cur_batch,\n",
    "                                                        shuffle=False,\n",
    "                                                        num_workers=NUM_WORKERS)\n",
    "       \n",
    "        train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n",
    "        predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n",
    "        print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n",
    "\n",
    "        if epoch_scheduler is not None:\n",
    "            epoch_scheduler.step(rmspe)\n",
    "\n",
    "        if rmspe < best_loss:\n",
    "            print(f'new best:{rmspe}')\n",
    "            best_loss = rmspe\n",
    "            best_prediction = predictions\n",
    "            model_save_path = DATA_DIR + \"/\" + os.path.join(output_dir, model_path.format(0))\n",
    "            torch.save(model, model_save_path)\n",
    "\n",
    "    best_predictions.append(best_prediction)\n",
    "    best_losses.append(best_loss)\n",
    "    # del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "    del train_dataset, valid_dataset, train_loader, valid_loader, opt\n",
    "    if scheduler is not None:\n",
    "        del scheduler\n",
    "    gc.collect()\n",
    "    # , scaler\n",
    "    return model, best_losses, best_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c3cb87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device mps\n"
     ]
    }
   ],
   "source": [
    "# get_device_name\n",
    "def get_device_name():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    \n",
    "    return \"cpu\"\n",
    "device = torch.device(get_device_name())\n",
    "print('device', device)\n",
    "\n",
    "# del df, df_train\n",
    "gc.collect()\n",
    "\n",
    "def get_top_n_models(models, scores, top_n):\n",
    "    if len(models) <= top_n:\n",
    "        print('number of models are less than top_n. all models will be used')\n",
    "        return models\n",
    "    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n",
    "    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n",
    "    return [x for _, x in sorted_][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12245cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-23T03:35:57.029334Z",
     "iopub.status.busy": "2022-01-23T03:35:56.898214Z",
     "iopub.status.idle": "2022-01-23T06:56:27.693883Z",
     "shell.execute_reply": "2022-01-23T06:56:27.695021Z",
     "shell.execute_reply.started": "2022-01-15T04:57:17.584692Z"
    },
    "papermill": {
     "duration": 12030.930352,
     "end_time": "2022-01-23T06:56:27.695345",
     "exception": false,
     "start_time": "2022-01-23T03:35:56.764993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_results_for_mlp\n",
    "def add_results_for_mlp(X_train_mlp, y_train_mlp, X_val_mlp, y_val_mlp, X_test_mlp, y_test_mlp, epochs, feature, lr = 0.002):\n",
    "    start_time = datetime.now()\n",
    "    model_mlp, nn_losses, nn_preds = train_nn(\n",
    "                                            X_train_mlp,\n",
    "                                            y_train_mlp,\n",
    "                                            X_val_mlp,\n",
    "                                            y_val_mlp,\n",
    "                                            device=device, \n",
    "                                            batch_size=512,\n",
    "                                            mlp_bn=True,\n",
    "                                            mlp_hidden=256,\n",
    "                                            mlp_dropout=0.0,\n",
    "                                            emb_dim=30,\n",
    "                                            epochs=epochs,\n",
    "                                            lr=lr,\n",
    "                                            max_lr=0.0055,\n",
    "                                            weight_decay=1e-7,\n",
    "                                            model_path='mlp_fold_{}' + f\"_seed{SEED}.pth\",\n",
    "                                            seed=0)\n",
    "\n",
    "    model_mlp_preds = predict_nn(X_test_mlp, model_mlp, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "    end_time = datetime.now()\n",
    "    add_model_result('MLP', y_test_mlp, model_mlp_preds, False, feature, end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da4acd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_results_for_cnn\n",
    "def add_results_for_cnn(X_train_cnn, y_train_cnn, X_val_cnn, y_val_cnn, X_test_cnn, y_test_cnn, epochs, feature, lr = 0.00038):\n",
    "    start_time = datetime.now()\n",
    "    model_cnn, nn_losses, nn_preds = train_nn(\n",
    "                                            X_train_cnn,\n",
    "                                            y_train_cnn,\n",
    "                                            X_val_cnn,\n",
    "                                            y_val_cnn,\n",
    "                                            device=device, \n",
    "                                            cnn_hidden=8*128,\n",
    "                                            batch_size=1280,\n",
    "                                            model_type='cnn',\n",
    "                                            emb_dim=30,\n",
    "                                            epochs=epochs,\n",
    "                                            cnn_channel1=128,\n",
    "                                            cnn_channel2=3*128,\n",
    "                                            cnn_channel3=3*128,\n",
    "                                            lr=lr, #0.0011,\n",
    "                                            max_lr=0.0013,\n",
    "                                            weight_decay=6.5e-6,\n",
    "                                            optimizer_type='adam',\n",
    "                                            scheduler_type='reduce',\n",
    "                                            model_path='cnn_fold_{}' + f\"_seed{SEED}.pth\",\n",
    "                                            seed=0,\n",
    "                                            cnn_dropout=0.0,\n",
    "                                            cnn_weight_norm=False, # Note: True\n",
    "                                            cnn_leaky_relu=False,\n",
    "                                            patience=8,\n",
    "                                            factor=0.3)\n",
    "\n",
    "    model_cnn_preds = predict_nn(X_test_cnn, model_cnn, device, ensemble_method=ENSEMBLE_METHOD)\n",
    "    time_taken = datetime.now() - start_time\n",
    "    add_model_result('CNN', y_test_cnn, model_cnn_preds, False, feature, time_taken)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "834277e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_timeseries_data\n",
    "def create_timeseries_data(df):\n",
    "    df_ts = TimeSeries.from_dataframe(df)\n",
    "    scaler = Scaler()\n",
    "    df_ts = scaler.fit_transform(df_ts).astype(np.float32)\n",
    "    print('Length of Timeseries ', len(df_ts))\n",
    "    return df_ts, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef939fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_results_for_TCN\n",
    "def add_results_for_TCN(X_train_ts_tcn, y_train_ts_tcn, X_val_ts_tcn, y_val_ts_tcn, X_test_ts_tcn, y_test_ts_tcn, epochs, feature):\n",
    "    time_start = datetime.now()\n",
    "    model_tcn = TCNModel(\n",
    "        input_chunk_length=72,\n",
    "        output_chunk_length=36,\n",
    "        n_epochs=epochs, #500\n",
    "        dropout=0.1,\n",
    "        dilation_base=2,\n",
    "        weight_norm=True,\n",
    "        kernel_size=3,\n",
    "        num_filters=3,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    model_tcn.fit(\n",
    "        series=y_train_ts_tcn,\n",
    "        past_covariates=X_train_ts_tcn,\n",
    "        val_series=y_val_ts_tcn,\n",
    "        val_past_covariates=X_val_ts_tcn,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    backtest_tcn = model_tcn.historical_forecasts(\n",
    "        series=y_test_ts_tcn,\n",
    "        past_covariates=X_test_ts_tcn,\n",
    "        forecast_horizon=36,\n",
    "        retrain=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    time_taken = datetime.now() - time_start\n",
    "    add_model_result('TCN', y_test_ts_tcn, backtest_tcn, True, feature, time_taken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9888c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_tcn_predictions\n",
    "def plot_tcn_predictions(y_test_ts_tcn, backtest_tcn):\n",
    "    y_test_ts_tcn.plot(label=\"actual\")\n",
    "    backtest_tcn.plot(label=\"backtest (H=6)\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e1a6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_results_for_lstm\n",
    "def add_results_for_lstm(X_train_ts_lstm, y_train_ts_lstm, X_val_ts_lstm, y_val_ts_lstm, X_test_ts_lstm, y_test_ts_lstm, epochs, feature):\n",
    "    start_time = datetime.now()\n",
    "    model_lstm = RNNModel(\n",
    "        model=\"LSTM\",\n",
    "        hidden_dim=20,\n",
    "        n_rnn_layers=2,\n",
    "        dropout=0.2,\n",
    "        batch_size=16,\n",
    "        n_epochs=epochs,\n",
    "        optimizer_kwargs={\"lr\": 1e-3},\n",
    "        random_state=0,\n",
    "        training_length=300,\n",
    "        input_chunk_length=300,\n",
    "        likelihood=GaussianLikelihood(),\n",
    "    )\n",
    "\n",
    "    model_lstm.fit(\n",
    "        series=y_train_ts_lstm,\n",
    "        future_covariates=X_train_ts_lstm,\n",
    "        val_series=y_val_ts_lstm,\n",
    "        val_future_covariates=X_val_ts_lstm,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    backtest_lstm = model_lstm.historical_forecasts(\n",
    "        series=y_test_ts_lstm,\n",
    "        future_covariates=X_test_ts_lstm,\n",
    "        forecast_horizon=36,\n",
    "        retrain=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    time_taken = datetime.now() - start_time\n",
    "    add_model_result('LSTM', y_test_ts_lstm, backtest_lstm, True, feature, time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6b007a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_lstm_predictions\n",
    "def plot_lstm_predictions(y_test_ts_lstm, backtest_lstm):\n",
    "    y_test_ts_lstm.plot(label=\"actual\")\n",
    "    backtest_lstm.plot(label=\"backtest (H=6)\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bb97e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_results_for_transformer\n",
    "def add_results_for_transformer(X_train_ts_trans, y_train_ts_trans, X_val_ts_trans, y_val_ts_trans, X_test_ts_trans, y_test_ts_trans, epochs, feature):\n",
    "    start_time = datetime.now()\n",
    "    model_transformer = TransformerModel(\n",
    "        input_chunk_length=12,\n",
    "        output_chunk_length=1,\n",
    "        batch_size=32,\n",
    "        n_epochs=epochs,\n",
    "        model_name=\"air_transformer\",\n",
    "        nr_epochs_val_period=10,\n",
    "        d_model=16,\n",
    "        nhead=8,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        activation=\"relu\",\n",
    "        random_state=42,\n",
    "        save_checkpoints=True,\n",
    "        force_reset=True,\n",
    "    )\n",
    "\n",
    "    model_transformer.fit(\n",
    "        series=y_train_ts_trans,\n",
    "        past_covariates=X_train_ts_trans,\n",
    "        val_series=y_val_ts_trans,\n",
    "        val_past_covariates=X_val_ts_trans,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    backtest_transformer = model_transformer.historical_forecasts(\n",
    "        series=y_test_ts_trans,\n",
    "        past_covariates=X_test_ts_trans,\n",
    "        forecast_horizon=36,\n",
    "        retrain=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    time_taken = datetime.now() - start_time\n",
    "    add_model_result('Transformer', y_test_ts_trans, backtest_transformer, True, feature, time_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a622cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform_experiments_multivariate\n",
    "def perform_experiments_multivariate(df_experiment, feature: str):\n",
    "    df_train, df_validation, df_test = split_df_into_train_val_test(df_experiment)\n",
    "\n",
    "    # prepare train, validation and test data\n",
    "    X_train = get_X(df_train)\n",
    "    X_val = get_X(df_validation)\n",
    "    X_test = get_X(df_test)\n",
    "\n",
    "    y_train = df_train['target']\n",
    "    y_val = df_validation['target']\n",
    "    y_test = df_test['target']\n",
    "\n",
    "    X_train_ts, X_train_ts_scaler = create_timeseries_data(X_train)\n",
    "    X_val_ts, X_val_ts_scaler = create_timeseries_data(X_val)\n",
    "    X_test_ts, X_test_ts_scaler = create_timeseries_data(X_test)\n",
    "\n",
    "    y_train_ts, y_train_ts_scaler = create_timeseries_data(y_train.to_frame())\n",
    "    y_val_ts, y_val_ts_scaler = create_timeseries_data(y_val.to_frame())\n",
    "    y_test_ts, y_test_ts_scaler = create_timeseries_data(y_test.to_frame())\n",
    "\n",
    "    # X_ts = X_train_ts.append(X_val_ts)\n",
    "    # y_ts = y_train_ts.append(y_val_ts)\n",
    "\n",
    "    print('X_train.shape ', X_train.shape)\n",
    "    print('X_val.shape ', X_val.shape)\n",
    "    print('X_test.shape ', X_test.shape)\n",
    "    print('y_train.shape ', y_train.shape)\n",
    "    print('y_val.shape ', y_val.shape)\n",
    "    print('y_test.shape ', y_test.shape)\n",
    "\n",
    "    # models\n",
    "    reset_model_results()\n",
    "    add_results_from_light_gbm(X_train, y_train, X_val, y_val, feature, lr=0.3)\n",
    "    add_results_for_mlp(X_train, y_train, X_val, y_val, X_test, y_test, EPOCHS, feature, lr = 0.002)\n",
    "    add_results_for_cnn(X_train, y_train, X_val, y_val, X_test, y_test, EPOCHS, feature, lr = 0.00038)\n",
    "    add_results_for_TCN(X_train_ts, y_train_ts, X_val_ts, y_val_ts, X_test_ts, y_test_ts, EPOCHS, feature)\n",
    "    add_results_for_lstm(X_train_ts, y_train_ts, X_val_ts, y_val_ts, X_test_ts, y_test_ts, EPOCHS, feature)\n",
    "    add_results_for_transformer(X_train_ts, y_train_ts, X_val_ts, y_val_ts, X_test_ts, y_test_ts, EPOCHS, feature)\n",
    "    \n",
    "    return get_model_results_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f956eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.shape  (428932, 3)\n",
      "stock_ids  112\n",
      "Train.shape  (3830, 3)\n",
      "stock_ids  {0}\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'train.csv'))\n",
    "stock_ids = set(train['stock_id'])\n",
    "print('Train.shape ', train.shape)\n",
    "print('stock_ids ', len(stock_ids))\n",
    "\n",
    "stock_ids_to_include = [0]\n",
    "train = train[train['stock_id'].isin(stock_ids_to_include)]\n",
    "print('Train.shape ', train.shape)\n",
    "stock_ids = set(train['stock_id'])\n",
    "print('stock_ids ', stock_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "914aadf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[books] 5.762sec\n",
      "[trades] 0.728sec\n",
      "[extra features] 0.007sec\n",
      "book path  /Users/pujanmaharjan/uni adelaide/uofa_research_project/datasets/optiver-realized-volatility-prediction/book_train.parquet/**/*.parquet\n",
      "[load files] 0.024sec\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=4)]: Done  22 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done  31 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=4)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=4)]: Done  35 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=4)]: Done  37 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=4)]: Done  38 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done  39 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=4)]: Done  41 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=4)]: Done  43 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=4)]: Done  45 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=4)]: Done  46 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=4)]: Done  47 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  50 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=4)]: Done  51 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  52 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=4)]: Done  54 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=4)]: Done  55 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done  57 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=4)]: Done  59 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=4)]: Done  61 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=4)]: Done  62 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=4)]: Done  63 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=4)]: Done  65 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done  66 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done  67 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=4)]: Done  69 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=4)]: Done  70 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=4)]: Done  71 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=4)]: Done  72 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=4)]: Done  73 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=4)]: Done  74 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=4)]: Done  75 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=4)]: Done  78 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=4)]: Done  79 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=4)]: Done  80 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=4)]: Done  81 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=4)]: Done  82 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=4)]: Done  83 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=4)]: Done  84 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=4)]: Done  85 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=4)]: Done  86 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=4)]: Done  87 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=4)]: Done  89 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   10.7s\n",
      "[Parallel(n_jobs=4)]: Done  91 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=4)]: Done  92 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=4)]: Done  93 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=4)]: Done  94 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=4)]: Done  95 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=4)]: Done  96 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=4)]: Done  97 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=4)]: Done  98 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=4)]: Done  99 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=4)]: Done 100 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=4)]: Done 101 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=4)]: Done 102 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=4)]: Done 103 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=4)]: Done 104 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=4)]: Done 108 out of 112 | elapsed:   13.0s remaining:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 112 out of 112 | elapsed:   13.4s finished\n",
      "[calc prices] 13.625sec\n",
      "[t-SNE(400) -> 50] 201.146sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "      <th>book.wap1.sum</th>\n",
       "      <th>book.wap2.sum</th>\n",
       "      <th>book.log_return1.realized_volatility</th>\n",
       "      <th>book.log_return2.realized_volatility</th>\n",
       "      <th>book.log_return_ask1.realized_volatility</th>\n",
       "      <th>book.log_return_ask2.realized_volatility</th>\n",
       "      <th>book.log_return_bid1.realized_volatility</th>\n",
       "      <th>book.log_return_bid2.realized_volatility</th>\n",
       "      <th>trade.log_return.realized_volatility</th>\n",
       "      <th>trade.seconds_in_bucket.count</th>\n",
       "      <th>trade.size.sum</th>\n",
       "      <th>trade.order_count.mean</th>\n",
       "      <th>time_id_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4294</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>185.004046</td>\n",
       "      <td>184.996116</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>14</td>\n",
       "      <td>2034</td>\n",
       "      <td>3.857143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>24033</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>340.516826</td>\n",
       "      <td>340.519624</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>26</td>\n",
       "      <td>1755</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5666</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>294.704533</td>\n",
       "      <td>294.708897</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>29</td>\n",
       "      <td>1313</td>\n",
       "      <td>1.965517</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>29740</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>206.123983</td>\n",
       "      <td>206.132851</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>26</td>\n",
       "      <td>1701</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22178</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>239.603879</td>\n",
       "      <td>239.602212</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.003641</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>21</td>\n",
       "      <td>2017</td>\n",
       "      <td>2.523810</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>0</td>\n",
       "      <td>24913</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>175.679981</td>\n",
       "      <td>175.668721</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>26</td>\n",
       "      <td>3225</td>\n",
       "      <td>5.461538</td>\n",
       "      <td>3825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3826</th>\n",
       "      <td>0</td>\n",
       "      <td>32195</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>307.699095</td>\n",
       "      <td>307.704473</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>15</td>\n",
       "      <td>1431</td>\n",
       "      <td>4.266667</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3827</th>\n",
       "      <td>0</td>\n",
       "      <td>15365</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>185.384277</td>\n",
       "      <td>185.380768</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>26</td>\n",
       "      <td>1243</td>\n",
       "      <td>3.730769</td>\n",
       "      <td>3827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3828</th>\n",
       "      <td>0</td>\n",
       "      <td>10890</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>282.783980</td>\n",
       "      <td>282.782871</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>39</td>\n",
       "      <td>5954</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3829</th>\n",
       "      <td>0</td>\n",
       "      <td>29316</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>193.189315</td>\n",
       "      <td>193.192407</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.004340</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>31</td>\n",
       "      <td>3082</td>\n",
       "      <td>4.129032</td>\n",
       "      <td>3829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3830 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      stock_id  time_id    target  book.wap1.sum  book.wap2.sum  \\\n",
       "0            0     4294  0.003267     185.004046     184.996116   \n",
       "1            0    24033  0.002580     340.516826     340.519624   \n",
       "2            0     5666  0.002051     294.704533     294.708897   \n",
       "3            0    29740  0.002364     206.123983     206.132851   \n",
       "4            0    22178  0.001439     239.603879     239.602212   \n",
       "...        ...      ...       ...            ...            ...   \n",
       "3825         0    24913  0.002402     175.679981     175.668721   \n",
       "3826         0    32195  0.002311     307.699095     307.704473   \n",
       "3827         0    15365  0.002017     185.384277     185.380768   \n",
       "3828         0    10890  0.003475     282.783980     282.782871   \n",
       "3829         0    29316  0.002846     193.189315     193.192407   \n",
       "\n",
       "      book.log_return1.realized_volatility  \\\n",
       "0                                 0.007026   \n",
       "1                                 0.004136   \n",
       "2                                 0.002395   \n",
       "3                                 0.001790   \n",
       "4                                 0.002601   \n",
       "...                                    ...   \n",
       "3825                              0.001588   \n",
       "3826                              0.002462   \n",
       "3827                              0.002390   \n",
       "3828                              0.003142   \n",
       "3829                              0.002736   \n",
       "\n",
       "      book.log_return2.realized_volatility  \\\n",
       "0                                 0.010722   \n",
       "1                                 0.005465   \n",
       "2                                 0.003925   \n",
       "3                                 0.003601   \n",
       "4                                 0.003641   \n",
       "...                                    ...   \n",
       "3825                              0.003124   \n",
       "3826                              0.003973   \n",
       "3827                              0.003386   \n",
       "3828                              0.004277   \n",
       "3829                              0.004340   \n",
       "\n",
       "      book.log_return_ask1.realized_volatility  \\\n",
       "0                                     0.004446   \n",
       "1                                     0.002225   \n",
       "2                                     0.001526   \n",
       "3                                     0.001658   \n",
       "4                                     0.001664   \n",
       "...                                        ...   \n",
       "3825                                  0.001637   \n",
       "3826                                  0.001134   \n",
       "3827                                  0.001577   \n",
       "3828                                  0.001730   \n",
       "3829                                  0.001573   \n",
       "\n",
       "      book.log_return_ask2.realized_volatility  \\\n",
       "0                                     0.004511   \n",
       "1                                     0.002461   \n",
       "2                                     0.002125   \n",
       "3                                     0.001665   \n",
       "4                                     0.002367   \n",
       "...                                        ...   \n",
       "3825                                  0.001586   \n",
       "3826                                  0.001311   \n",
       "3827                                  0.001429   \n",
       "3828                                  0.001558   \n",
       "3829                                  0.001381   \n",
       "\n",
       "      book.log_return_bid1.realized_volatility  \\\n",
       "0                                     0.004145   \n",
       "1                                     0.002483   \n",
       "2                                     0.001539   \n",
       "3                                     0.001442   \n",
       "4                                     0.002133   \n",
       "...                                        ...   \n",
       "3825                                  0.000880   \n",
       "3826                                  0.001147   \n",
       "3827                                  0.001359   \n",
       "3828                                  0.001662   \n",
       "3829                                  0.001685   \n",
       "\n",
       "      book.log_return_bid2.realized_volatility  \\\n",
       "0                                     0.009287   \n",
       "1                                     0.002859   \n",
       "2                                     0.002011   \n",
       "3                                     0.001747   \n",
       "4                                     0.002148   \n",
       "...                                        ...   \n",
       "3825                                  0.001403   \n",
       "3826                                  0.001310   \n",
       "3827                                  0.001949   \n",
       "3828                                  0.001805   \n",
       "3829                                  0.002205   \n",
       "\n",
       "      trade.log_return.realized_volatility  trade.seconds_in_bucket.count  \\\n",
       "0                                 0.003655                             14   \n",
       "1                                 0.001459                             26   \n",
       "2                                 0.000801                             29   \n",
       "3                                 0.000861                             26   \n",
       "4                                 0.001169                             21   \n",
       "...                                    ...                            ...   \n",
       "3825                              0.001631                             26   \n",
       "3826                              0.001381                             15   \n",
       "3827                              0.001234                             26   \n",
       "3828                              0.001697                             39   \n",
       "3829                              0.001302                             31   \n",
       "\n",
       "      trade.size.sum  trade.order_count.mean  time_id_order  \n",
       "0               2034                3.857143              0  \n",
       "1               1755                1.807692              1  \n",
       "2               1313                1.965517              2  \n",
       "3               1701                1.807692              3  \n",
       "4               2017                2.523810              4  \n",
       "...              ...                     ...            ...  \n",
       "3825            3225                5.461538           3825  \n",
       "3826            1431                4.266667           3826  \n",
       "3827            1243                3.730769           3827  \n",
       "3828            5954                4.384615           3828  \n",
       "3829            3082                4.129032           3829  \n",
       "\n",
       "[3830 rows x 16 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_order_df = make_features(train, DataBlock.TRAIN)\n",
    "book_order_df = add_time_id_order(book_order_df)\n",
    "book_order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cc5d5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data shape  (3830, 16)\n",
      "train shape  (2681, 16)\n",
      "validation shape  (574, 16)\n",
      "test shape  (575, 16)\n",
      "Length of Timeseries  2681\n",
      "Length of Timeseries  574\n",
      "Length of Timeseries  575\n",
      "Length of Timeseries  2681\n",
      "Length of Timeseries  574\n",
      "Length of Timeseries  575\n",
      "X_train.shape  (2681, 14)\n",
      "X_val.shape  (574, 14)\n",
      "X_test.shape  (575, 14)\n",
      "y_train.shape  (2681,)\n",
      "y_val.shape  (574,)\n",
      "y_test.shape  (575,)\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tcv_agg's l2: 1.04551e-06 + 8.50765e-08\tcv_agg's RMSPE: 0.386808 + 0.012064\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tcv_agg's l2: 1.03284e-06 + 8.19042e-08\tcv_agg's RMSPE: 0.384488 + 0.0119465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tcv_agg's l2: 1.02756e-06 + 8.23267e-08\tcv_agg's RMSPE: 0.383498 + 0.0122424\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tcv_agg's l2: 1.02546e-06 + 8.1752e-08\tcv_agg's RMSPE: 0.383116 + 0.0123236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[100]\tcv_agg's l2: 1.02486e-06 + 8.14514e-08\tcv_agg's RMSPE: 0.38301 + 0.0123449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "y_pred here \n",
      "mae value  0.0008734400003664653\n",
      "y_pred here \n",
      "mae value  0.0008608195368535222\n",
      "updating best mae value\n",
      "y_pred here \n",
      "mae value  0.0008807674203865313\n",
      "y_pred here \n",
      "mae value  0.0008698054912060968\n",
      "y_pred here \n",
      "mae value  0.0008661369929232168\n",
      "using sklearn metrics\n",
      "adding new model results in\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdec3bed6f443689c14a86bc7014078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91c730690c6465785e407ba13a381c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 291.225, valid rmspe: 274.366\n",
      "new best:274.36578369140625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba7e9854f6a4b238315ad08dc38ff37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b52ac7d8d87411cbef9929ebabda96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss: 70.312, valid rmspe: 101.395\n",
      "new best:101.3949203491211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0acce5274042f3adbe240a526e5964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sklearn metrics\n",
      "adding new model results in\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c1e6f0a48140bc8f432a5fd65cf85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a015b2165f455692e910ce5ab55f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 489.642, valid rmspe: 174.145\n",
      "new best:174.14483642578125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9522b5644540b0bb72243f1c47ba10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0461fdf575b42698e66ee2ca75b1e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss: 544.472, valid rmspe: 248.733\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd843f79a4f64007900ec45a8591fd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sklearn metrics\n",
      "adding new model results in\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TCNModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m perform_experiments_multivariate(book_order_df, \u001b[39m'\u001b[39;49m\u001b[39mlog_return\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[62], line 37\u001b[0m, in \u001b[0;36mperform_experiments_multivariate\u001b[0;34m(df_experiment, feature)\u001b[0m\n\u001b[1;32m     35\u001b[0m add_results_for_mlp(X_train, y_train, X_val, y_val, X_test, y_test, EPOCHS, feature, lr \u001b[39m=\u001b[39m \u001b[39m0.002\u001b[39m)\n\u001b[1;32m     36\u001b[0m add_results_for_cnn(X_train, y_train, X_val, y_val, X_test, y_test, EPOCHS, feature, lr \u001b[39m=\u001b[39m \u001b[39m0.00038\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m add_results_for_TCN(X_train_ts, y_train_ts, X_val_ts, y_val_ts, X_test_ts, y_test_ts, EPOCHS, feature)\n\u001b[1;32m     38\u001b[0m add_results_for_lstm(X_train_ts, y_train_ts, X_val_ts, y_val_ts, X_test_ts, y_test_ts, EPOCHS, feature)\n\u001b[1;32m     39\u001b[0m add_results_for_transformer(X_train_ts, y_train_ts, X_val_ts, y_val_ts, X_test_ts, y_test_ts, EPOCHS, feature)\n",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m, in \u001b[0;36madd_results_for_TCN\u001b[0;34m(X_train_ts_tcn, y_train_ts_tcn, X_val_ts_tcn, y_val_ts_tcn, X_test_ts_tcn, y_test_ts_tcn, epochs, feature)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_results_for_TCN\u001b[39m(X_train_ts_tcn, y_train_ts_tcn, X_val_ts_tcn, y_val_ts_tcn, X_test_ts_tcn, y_test_ts_tcn, epochs, feature):\n\u001b[1;32m      3\u001b[0m     time_start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m----> 4\u001b[0m     model_tcn \u001b[39m=\u001b[39m TCNModel(\n\u001b[1;32m      5\u001b[0m         input_chunk_length\u001b[39m=\u001b[39m\u001b[39m72\u001b[39m,\n\u001b[1;32m      6\u001b[0m         output_chunk_length\u001b[39m=\u001b[39m\u001b[39m36\u001b[39m,\n\u001b[1;32m      7\u001b[0m         n_epochs\u001b[39m=\u001b[39mepochs, \u001b[39m#500\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m      9\u001b[0m         dilation_base\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m     10\u001b[0m         weight_norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m         kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     12\u001b[0m         num_filters\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     13\u001b[0m         random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     model_tcn\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     17\u001b[0m         series\u001b[39m=\u001b[39my_train_ts_tcn,\n\u001b[1;32m     18\u001b[0m         past_covariates\u001b[39m=\u001b[39mX_train_ts_tcn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     backtest_tcn \u001b[39m=\u001b[39m model_tcn\u001b[39m.\u001b[39mhistorical_forecasts(\n\u001b[1;32m     25\u001b[0m         series\u001b[39m=\u001b[39my_test_ts_tcn,\n\u001b[1;32m     26\u001b[0m         past_covariates\u001b[39m=\u001b[39mX_test_ts_tcn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TCNModel' is not defined"
     ]
    }
   ],
   "source": [
    "perform_experiments_multivariate(book_order_df, 'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0505cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "book_trade_with_spread_features = make_features(train, DataBlock.TRAIN, add_spread_features=True)\n",
    "print(book_trade_with_spread_features.columns)\n",
    "perform_experiments_multivariate(book_trade_with_spread_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_PRECOMPUTE_FEATURES = False\n",
    "# print('USE_PRECOMPUTE_FEATURES ', USE_PRECOMPUTE_FEATURES)\n",
    "# if USE_PRECOMPUTE_FEATURES:\n",
    "#     with timer('load feather'):\n",
    "#         df = pd.read_feather(DATA_DIR + '/data-cache/features_v2.f')\n",
    "# else:\n",
    "#     print('making features ')\n",
    "#     df = make_features(train, DataBlock.TRAIN)\n",
    "#     # v2\n",
    "#     df = make_features_v2(df, DataBlock.TRAIN)\n",
    "\n",
    "    # df.to_feather(DATA_DIR + '/data-cache/features_v2.f')  # save cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timer('make nearest neighbor feature'):\n",
    "#     df_with_nn_features = make_nearest_neighbor_feature(df)\n",
    "\n",
    "# print(df_with_nn_features.shape)\n",
    "# df_with_nn_features.reset_index(drop=True).to_feather(DATA_DIR + '/data-cache/optiver_df2.f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chek_null_columns(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435af70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.fillna(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16517.385856,
   "end_time": "2022-01-23T07:01:10.301218",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-23T02:25:52.915362",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
